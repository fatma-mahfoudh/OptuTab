{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tune_models import tune_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(\"data\", \"pmsm_temperature_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"stator_yoke\"] = df[\"stator_yoke\"].map({\"Positive\":1, \"Negative\":0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ambient           0\n",
       "coolant           0\n",
       "u_d               0\n",
       "u_q               0\n",
       "motor_speed       1\n",
       "torque            1\n",
       "i_d               1\n",
       "i_q               1\n",
       "pm                1\n",
       "stator_yoke       1\n",
       "stator_winding    1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_targets = ['pm', 'stator_winding'] \n",
    "cat_targets = ['stator_yoke']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(num_targets+cat_targets, axis=1)\n",
    "y = df[num_targets+cat_targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\optuna\\samplers\\_tpe\\sampler.py:295: ExperimentalWarning: ``multivariate`` option is an experimental feature. The interface can change in the future.\n",
      "  warnings.warn(\n",
      "[I 2023-12-08 20:57:02,277] A new study created in memory with name: regression\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started tuning xgboost for pm...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-08 20:57:08,742] Trial 0 finished with value: 0.42094933333333334 and parameters: {'n_estimators': 7, 'learning_rate': 0.43775200014817905, 'subsample': 0.6, 'colsample_bytree': 1.0, 'min_child_weight': 0.8491983767203796, 'reg_alpha': 0.0070689749506246055, 'reg_lambda': 0.005337032762603957, 'max_leaves': 5, 'colsample_bylevel': 0.6521211214797689, 'max_depth': 5}. Best is trial 0 with value: 0.42094933333333334.\n",
      "[I 2023-12-08 20:57:14,770] Trial 1 finished with value: 1.050309 and parameters: {'n_estimators': 8, 'learning_rate': 0.00644285859857343, 'subsample': 0.6, 'colsample_bytree': 0.6, 'min_child_weight': 0.6467903667112945, 'reg_alpha': 0.004809461967501573, 'reg_lambda': 0.0018205657658407262, 'max_leaves': 20, 'colsample_bylevel': 0.9828160165372797, 'max_depth': 6}. Best is trial 0 with value: 0.42094933333333334.\n",
      "[I 2023-12-08 20:57:20,817] Trial 2 finished with value: 1.0929330000000002 and parameters: {'n_estimators': 6, 'learning_rate': 0.0018678754137262702, 'subsample': 0.6, 'colsample_bytree': 0.6, 'min_child_weight': 0.5920392514089517, 'reg_alpha': 0.0054880470007660455, 'reg_lambda': 7.556810141274429, 'max_leaves': 16, 'colsample_bylevel': 0.9697494707820946, 'max_depth': 6}. Best is trial 0 with value: 0.42094933333333334.\n",
      "[I 2023-12-08 20:57:25,882] Trial 3 finished with value: 0.404768 and parameters: {'n_estimators': 10, 'learning_rate': 0.3640039183945802, 'subsample': 1.0, 'colsample_bytree': 0.7, 'min_child_weight': 0.2268318024772864, 'reg_alpha': 1.6172900811143154, 'reg_lambda': 0.0019870215385428634, 'max_leaves': 20, 'colsample_bylevel': 0.8861223846483287, 'max_depth': 3}. Best is trial 3 with value: 0.404768.\n",
      "[I 2023-12-08 20:57:31,521] Trial 4 finished with value: 0.4105196666666666 and parameters: {'n_estimators': 3, 'learning_rate': 0.18427872005083668, 'subsample': 0.8, 'colsample_bytree': 0.7, 'min_child_weight': 0.379884089544096, 'reg_alpha': 0.01998634077852888, 'reg_lambda': 0.8287522363768158, 'max_leaves': 14, 'colsample_bylevel': 0.9436063712881633, 'max_depth': 4}. Best is trial 3 with value: 0.404768.\n",
      "[I 2023-12-08 20:57:34,358] Trial 5 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 20:57:37,381] Trial 6 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 20:57:40,543] Trial 7 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 20:57:43,236] Trial 8 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 20:57:46,082] Trial 9 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 20:57:49,007] Trial 10 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 20:57:51,920] Trial 11 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 20:57:57,085] Trial 12 finished with value: 0.39750100000000005 and parameters: {'n_estimators': 9, 'learning_rate': 0.36696344098105327, 'subsample': 1.0, 'colsample_bytree': 1.0, 'min_child_weight': 0.2366124135990546, 'reg_alpha': 0.646982881059252, 'reg_lambda': 0.002448330208782887, 'max_leaves': 15, 'colsample_bylevel': 0.8613244048362696, 'max_depth': 3}. Best is trial 12 with value: 0.39750100000000005.\n",
      "[I 2023-12-08 20:57:59,778] Trial 13 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 20:58:02,543] Trial 14 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 20:58:07,692] Trial 15 finished with value: 0.36581233333333335 and parameters: {'n_estimators': 10, 'learning_rate': 0.25649692086899134, 'subsample': 1.0, 'colsample_bytree': 0.7, 'min_child_weight': 0.4726061745044262, 'reg_alpha': 0.8103205553552708, 'reg_lambda': 0.0011736494006959503, 'max_leaves': 20, 'colsample_bylevel': 0.9656356941044635, 'max_depth': 4}. Best is trial 15 with value: 0.36581233333333335.\n",
      "[I 2023-12-08 20:58:12,640] Trial 16 finished with value: 0.2866056666666667 and parameters: {'n_estimators': 9, 'learning_rate': 0.5878752397781104, 'subsample': 1.0, 'colsample_bytree': 1.0, 'min_child_weight': 0.2931592055869374, 'reg_alpha': 1.047760164872906, 'reg_lambda': 0.0019661800992795044, 'max_leaves': 17, 'colsample_bylevel': 0.9971000983176577, 'max_depth': 4}. Best is trial 16 with value: 0.2866056666666667.\n",
      "[I 2023-12-08 20:58:17,569] Trial 17 finished with value: 0.363197 and parameters: {'n_estimators': 10, 'learning_rate': 0.25982652036531634, 'subsample': 1.0, 'colsample_bytree': 0.7, 'min_child_weight': 0.4871879711054177, 'reg_alpha': 6.765629053754249, 'reg_lambda': 0.005153545915806236, 'max_leaves': 18, 'colsample_bylevel': 0.9068182575220413, 'max_depth': 4}. Best is trial 16 with value: 0.2866056666666667.\n",
      "[I 2023-12-08 20:58:20,338] Trial 18 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 20:58:23,140] Trial 19 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 20:58:28,207] Trial 20 finished with value: 0.3161316666666667 and parameters: {'n_estimators': 10, 'learning_rate': 0.4392828144198504, 'subsample': 1.0, 'colsample_bytree': 0.8, 'min_child_weight': 0.5334447042211083, 'reg_alpha': 1.1674363995357042, 'reg_lambda': 0.007762254165632582, 'max_leaves': 16, 'colsample_bylevel': 0.9227658209785266, 'max_depth': 4}. Best is trial 16 with value: 0.2866056666666667.\n",
      "[I 2023-12-08 20:58:33,031] Trial 21 finished with value: 0.28044833333333336 and parameters: {'n_estimators': 12, 'learning_rate': 0.5717075200425372, 'subsample': 1.0, 'colsample_bytree': 1.0, 'min_child_weight': 0.31861279124143543, 'reg_alpha': 0.6137607849944652, 'reg_lambda': 0.004038127330669078, 'max_leaves': 17, 'colsample_bylevel': 0.9553492878046866, 'max_depth': 4}. Best is trial 21 with value: 0.28044833333333336.\n",
      "[I 2023-12-08 20:58:38,577] Trial 22 finished with value: 0.32062466666666667 and parameters: {'n_estimators': 12, 'learning_rate': 0.3659994385215518, 'subsample': 0.6, 'colsample_bytree': 1.0, 'min_child_weight': 0.318984329348571, 'reg_alpha': 6.324081146067438, 'reg_lambda': 0.0011302253392861384, 'max_leaves': 15, 'colsample_bylevel': 0.9924477890963765, 'max_depth': 4}. Best is trial 21 with value: 0.28044833333333336.\n",
      "[I 2023-12-08 20:58:41,304] Trial 23 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 20:58:46,258] Trial 24 finished with value: 0.294676 and parameters: {'n_estimators': 10, 'learning_rate': 0.4499014773173098, 'subsample': 1.0, 'colsample_bytree': 1.0, 'min_child_weight': 0.154155197285645, 'reg_alpha': 0.1622415369751394, 'reg_lambda': 0.012903920153364189, 'max_leaves': 19, 'colsample_bylevel': 0.9076455567502408, 'max_depth': 4}. Best is trial 21 with value: 0.28044833333333336.\n",
      "[I 2023-12-08 20:58:49,023] Trial 25 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 20:58:54,115] Trial 26 finished with value: 0.28904466666666667 and parameters: {'n_estimators': 12, 'learning_rate': 0.4879754878332141, 'subsample': 1.0, 'colsample_bytree': 1.0, 'min_child_weight': 0.3677047574885718, 'reg_alpha': 4.588389272744009, 'reg_lambda': 0.0018223421328420338, 'max_leaves': 17, 'colsample_bylevel': 0.902107383521439, 'max_depth': 5}. Best is trial 21 with value: 0.28044833333333336.\n",
      "[I 2023-12-08 20:58:56,866] Trial 27 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 20:59:02,028] Trial 28 finished with value: 0.32300433333333334 and parameters: {'n_estimators': 11, 'learning_rate': 0.4276099863922282, 'subsample': 1.0, 'colsample_bytree': 0.6, 'min_child_weight': 0.1806345466948504, 'reg_alpha': 0.3323552357529712, 'reg_lambda': 0.0027828403998947787, 'max_leaves': 15, 'colsample_bylevel': 0.9231011709417495, 'max_depth': 4}. Best is trial 21 with value: 0.28044833333333336.\n",
      "[I 2023-12-08 20:59:06,984] Trial 29 finished with value: 0.282874 and parameters: {'n_estimators': 14, 'learning_rate': 0.568072263766823, 'subsample': 1.0, 'colsample_bytree': 1.0, 'min_child_weight': 0.21165449793695565, 'reg_alpha': 3.4951245845557213, 'reg_lambda': 0.02255067548610651, 'max_leaves': 16, 'colsample_bylevel': 0.8819419789260192, 'max_depth': 6}. Best is trial 21 with value: 0.28044833333333336.\n",
      "[I 2023-12-08 20:59:09,760] Trial 30 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 20:59:15,367] Trial 31 finished with value: 0.28354833333333335 and parameters: {'n_estimators': 8, 'learning_rate': 0.5262973485117356, 'subsample': 0.8, 'colsample_bytree': 1.0, 'min_child_weight': 0.3523161019467344, 'reg_alpha': 1.1358464387503402, 'reg_lambda': 0.009496998138058039, 'max_leaves': 18, 'colsample_bylevel': 0.9576082009856219, 'max_depth': 4}. Best is trial 21 with value: 0.28044833333333336.\n",
      "[I 2023-12-08 20:59:18,224] Trial 32 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 20:59:23,280] Trial 33 finished with value: 0.282068 and parameters: {'n_estimators': 14, 'learning_rate': 0.5532514951400503, 'subsample': 1.0, 'colsample_bytree': 1.0, 'min_child_weight': 0.33724738909843793, 'reg_alpha': 3.076535205260149, 'reg_lambda': 0.01136845676903896, 'max_leaves': 18, 'colsample_bylevel': 0.9176151543665639, 'max_depth': 6}. Best is trial 21 with value: 0.28044833333333336.\n",
      "[I 2023-12-08 20:59:28,267] Trial 34 finished with value: 0.3028926666666667 and parameters: {'n_estimators': 13, 'learning_rate': 0.41762167305181297, 'subsample': 1.0, 'colsample_bytree': 1.0, 'min_child_weight': 0.29787145648848146, 'reg_alpha': 1.7432985171903626, 'reg_lambda': 0.005839563609688986, 'max_leaves': 16, 'colsample_bylevel': 0.9049194582573566, 'max_depth': 6}. Best is trial 21 with value: 0.28044833333333336.\n",
      "[I 2023-12-08 20:59:33,339] Trial 35 finished with value: 0.2971316666666666 and parameters: {'n_estimators': 12, 'learning_rate': 0.37513093296790084, 'subsample': 1.0, 'colsample_bytree': 1.0, 'min_child_weight': 0.4008779095418078, 'reg_alpha': 1.592239039455433, 'reg_lambda': 0.05061608960337311, 'max_leaves': 19, 'colsample_bylevel': 0.9647177094236693, 'max_depth': 6}. Best is trial 21 with value: 0.28044833333333336.\n",
      "[I 2023-12-08 20:59:39,091] Trial 36 finished with value: 0.301019 and parameters: {'n_estimators': 8, 'learning_rate': 0.35478512745332635, 'subsample': 0.8, 'colsample_bytree': 1.0, 'min_child_weight': 0.36639622482688966, 'reg_alpha': 1.8258713497495938, 'reg_lambda': 0.021180212903686258, 'max_leaves': 20, 'colsample_bylevel': 0.9188723233883394, 'max_depth': 5}. Best is trial 21 with value: 0.28044833333333336.\n",
      "[I 2023-12-08 20:59:43,945] Trial 37 finished with value: 0.2916703333333333 and parameters: {'n_estimators': 15, 'learning_rate': 0.5256429399766647, 'subsample': 1.0, 'colsample_bytree': 1.0, 'min_child_weight': 0.12871332946160297, 'reg_alpha': 2.2177994097350897, 'reg_lambda': 0.13095514810039358, 'max_leaves': 15, 'colsample_bylevel': 0.9586601444407953, 'max_depth': 6}. Best is trial 21 with value: 0.28044833333333336.\n",
      "[I 2023-12-08 20:59:49,719] Trial 38 finished with value: 0.2819576666666667 and parameters: {'n_estimators': 15, 'learning_rate': 0.4533526107972442, 'subsample': 0.6, 'colsample_bytree': 1.0, 'min_child_weight': 0.38734298292128927, 'reg_alpha': 0.5301174282982798, 'reg_lambda': 0.004740312011638527, 'max_leaves': 19, 'colsample_bylevel': 0.8751324666464901, 'max_depth': 6}. Best is trial 21 with value: 0.28044833333333336.\n",
      "[I 2023-12-08 20:59:52,511] Trial 39 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 20:59:55,155] Trial 40 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 20:59:57,982] Trial 41 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:00:00,994] Trial 42 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:00:03,987] Trial 43 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:00:06,698] Trial 44 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:00:09,665] Trial 45 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:00:12,709] Trial 46 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:00:15,443] Trial 47 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:00:20,332] Trial 48 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-08 21:00:23,093] Trial 49 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:00:28,673] Trial 50 pruned. Trial was pruned at iteration 2.\n",
      "[I 2023-12-08 21:00:31,529] Trial 51 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:00:36,662] Trial 52 finished with value: 0.27718333333333334 and parameters: {'n_estimators': 15, 'learning_rate': 0.4415934997551072, 'subsample': 1.0, 'colsample_bytree': 1.0, 'min_child_weight': 0.28617195136072104, 'reg_alpha': 2.22162441263174, 'reg_lambda': 0.010473918896431369, 'max_leaves': 19, 'colsample_bylevel': 0.9098619595434244, 'max_depth': 6}. Best is trial 52 with value: 0.27718333333333334.\n",
      "[I 2023-12-08 21:00:41,736] Trial 53 finished with value: 0.278627 and parameters: {'n_estimators': 14, 'learning_rate': 0.5785270749603163, 'subsample': 1.0, 'colsample_bytree': 1.0, 'min_child_weight': 0.32558377565682595, 'reg_alpha': 4.507048787722495, 'reg_lambda': 0.021670285920363293, 'max_leaves': 18, 'colsample_bylevel': 0.8474856659498206, 'max_depth': 6}. Best is trial 52 with value: 0.27718333333333334.\n",
      "[I 2023-12-08 21:00:44,494] Trial 54 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:00:47,412] Trial 55 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:00:50,225] Trial 56 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:00:55,691] Trial 57 finished with value: 0.28877166666666665 and parameters: {'n_estimators': 14, 'learning_rate': 0.5918882415791467, 'subsample': 0.6, 'colsample_bytree': 1.0, 'min_child_weight': 0.17718865318990343, 'reg_alpha': 0.8944353374169811, 'reg_lambda': 0.017432640079410613, 'max_leaves': 15, 'colsample_bylevel': 0.9990645727316418, 'max_depth': 6}. Best is trial 52 with value: 0.27718333333333334.\n",
      "[I 2023-12-08 21:01:00,738] Trial 58 finished with value: 0.28707333333333335 and parameters: {'n_estimators': 11, 'learning_rate': 0.4159050703780725, 'subsample': 1.0, 'colsample_bytree': 1.0, 'min_child_weight': 0.3455999686737612, 'reg_alpha': 0.6877769852015523, 'reg_lambda': 0.024634993117138123, 'max_leaves': 20, 'colsample_bylevel': 0.7489222402707738, 'max_depth': 6}. Best is trial 52 with value: 0.27718333333333334.\n",
      "[I 2023-12-08 21:01:03,443] Trial 59 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:01:06,365] Trial 60 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:01:09,224] Trial 61 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:01:15,592] Trial 62 finished with value: 0.29107466666666665 and parameters: {'n_estimators': 14, 'learning_rate': 0.514936081297111, 'subsample': 1.0, 'colsample_bytree': 1.0, 'min_child_weight': 0.3067465792213648, 'reg_alpha': 0.5693088058536936, 'reg_lambda': 0.007980010939859828, 'max_leaves': 16, 'colsample_bylevel': 0.816821478752195, 'max_depth': 6}. Best is trial 52 with value: 0.27718333333333334.\n",
      "[I 2023-12-08 21:01:19,664] Trial 63 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:01:23,013] Trial 64 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:01:28,240] Trial 65 finished with value: 0.2859043333333333 and parameters: {'n_estimators': 15, 'learning_rate': 0.46241242342653666, 'subsample': 1.0, 'colsample_bytree': 1.0, 'min_child_weight': 0.19472548138615378, 'reg_alpha': 2.763046828125934, 'reg_lambda': 0.002470565231971966, 'max_leaves': 17, 'colsample_bylevel': 0.8254504987627578, 'max_depth': 5}. Best is trial 52 with value: 0.27718333333333334.\n",
      "[I 2023-12-08 21:01:33,905] Trial 66 finished with value: 0.2895276666666667 and parameters: {'n_estimators': 15, 'learning_rate': 0.4595761766056145, 'subsample': 0.6, 'colsample_bytree': 1.0, 'min_child_weight': 0.39483482395896585, 'reg_alpha': 1.3260674674763664, 'reg_lambda': 0.001491327925867793, 'max_leaves': 18, 'colsample_bylevel': 0.8639843790580022, 'max_depth': 6}. Best is trial 52 with value: 0.27718333333333334.\n",
      "[I 2023-12-08 21:01:37,586] Trial 67 pruned. Trial was pruned at iteration 3.\n",
      "[I 2023-12-08 21:01:40,596] Trial 68 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:01:46,375] Trial 69 finished with value: 0.275355 and parameters: {'n_estimators': 13, 'learning_rate': 0.43849856104233786, 'subsample': 0.7, 'colsample_bytree': 1.0, 'min_child_weight': 0.4072560865614377, 'reg_alpha': 1.253463013962474, 'reg_lambda': 0.004300419939160534, 'max_leaves': 20, 'colsample_bylevel': 0.9312651653947628, 'max_depth': 6}. Best is trial 69 with value: 0.275355.\n",
      "[I 2023-12-08 21:01:51,511] Trial 70 finished with value: 0.274551 and parameters: {'n_estimators': 15, 'learning_rate': 0.5307589575406952, 'subsample': 1.0, 'colsample_bytree': 1.0, 'min_child_weight': 0.4213432801615697, 'reg_alpha': 3.1181609776856103, 'reg_lambda': 0.013306359718244445, 'max_leaves': 19, 'colsample_bylevel': 0.8739189076006696, 'max_depth': 6}. Best is trial 70 with value: 0.274551.\n",
      "[I 2023-12-08 21:01:54,210] Trial 71 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:01:59,101] Trial 72 finished with value: 0.27663733333333335 and parameters: {'n_estimators': 13, 'learning_rate': 0.5880925064589557, 'subsample': 1.0, 'colsample_bytree': 1.0, 'min_child_weight': 0.3752248609217153, 'reg_alpha': 9.124202909041554, 'reg_lambda': 0.03765293277358798, 'max_leaves': 17, 'colsample_bylevel': 0.8275424574324812, 'max_depth': 6}. Best is trial 70 with value: 0.274551.\n",
      "[I 2023-12-08 21:02:01,934] Trial 73 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:02:06,876] Trial 74 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-08 21:02:09,946] Trial 75 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:02:12,973] Trial 76 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-08 21:02:15,661] Trial 77 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:02:21,048] Trial 78 finished with value: 0.28876266666666667 and parameters: {'n_estimators': 14, 'learning_rate': 0.4841094793936441, 'subsample': 1.0, 'colsample_bytree': 1.0, 'min_child_weight': 0.4274759698773455, 'reg_alpha': 9.504256815940018, 'reg_lambda': 0.013601799929245617, 'max_leaves': 17, 'colsample_bylevel': 0.8843258562737941, 'max_depth': 6}. Best is trial 70 with value: 0.274551.\n",
      "[I 2023-12-08 21:02:23,866] Trial 79 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:02:26,551] Trial 80 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:02:31,914] Trial 81 finished with value: 0.27806733333333333 and parameters: {'n_estimators': 11, 'learning_rate': 0.5481464322783497, 'subsample': 0.7, 'colsample_bytree': 1.0, 'min_child_weight': 0.3985032347161979, 'reg_alpha': 0.690457391294109, 'reg_lambda': 0.0017055677379496307, 'max_leaves': 17, 'colsample_bylevel': 0.9247288312518053, 'max_depth': 5}. Best is trial 70 with value: 0.274551.\n",
      "[I 2023-12-08 21:02:34,620] Trial 82 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:02:37,466] Trial 83 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:02:40,396] Trial 84 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:02:43,111] Trial 85 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:02:45,893] Trial 86 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:02:51,833] Trial 87 finished with value: 0.2695433333333333 and parameters: {'n_estimators': 14, 'learning_rate': 0.5463013975469231, 'subsample': 0.6, 'colsample_bytree': 1.0, 'min_child_weight': 0.37017478505009016, 'reg_alpha': 0.08889168229827807, 'reg_lambda': 0.020293665241204992, 'max_leaves': 20, 'colsample_bylevel': 0.9686264924331296, 'max_depth': 6}. Best is trial 87 with value: 0.2695433333333333.\n",
      "[I 2023-12-08 21:02:54,787] Trial 88 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:03:00,899] Trial 89 finished with value: 0.28382766666666664 and parameters: {'n_estimators': 13, 'learning_rate': 0.457442565216113, 'subsample': 0.6, 'colsample_bytree': 1.0, 'min_child_weight': 0.518464075743404, 'reg_alpha': 0.06973286014797206, 'reg_lambda': 0.010986465422631696, 'max_leaves': 19, 'colsample_bylevel': 0.8652139246811426, 'max_depth': 6}. Best is trial 87 with value: 0.2695433333333333.\n",
      "[I 2023-12-08 21:03:03,749] Trial 90 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:03:09,362] Trial 91 finished with value: 0.26941966666666667 and parameters: {'n_estimators': 13, 'learning_rate': 0.5701316356829403, 'subsample': 0.7, 'colsample_bytree': 1.0, 'min_child_weight': 0.5569597742653052, 'reg_alpha': 0.7762600649457623, 'reg_lambda': 0.0011283700373524246, 'max_leaves': 20, 'colsample_bylevel': 0.9534668720503621, 'max_depth': 6}. Best is trial 91 with value: 0.26941966666666667.\n",
      "[I 2023-12-08 21:03:12,269] Trial 92 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:03:15,226] Trial 93 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:03:18,141] Trial 94 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:03:21,240] Trial 95 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:03:23,986] Trial 96 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:03:26,701] Trial 97 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:03:29,466] Trial 98 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:03:35,231] Trial 99 finished with value: 0.2693426666666667 and parameters: {'n_estimators': 13, 'learning_rate': 0.5934633937691666, 'subsample': 0.7, 'colsample_bytree': 0.9, 'min_child_weight': 0.4448063945624003, 'reg_alpha': 0.4887294902572273, 'reg_lambda': 0.007612527315979616, 'max_leaves': 19, 'colsample_bylevel': 0.9821058980980604, 'max_depth': 6}. Best is trial 99 with value: 0.2693426666666667.\n",
      "[I 2023-12-08 21:03:40,708] Trial 100 finished with value: 0.2844413333333333 and parameters: {'n_estimators': 13, 'learning_rate': 0.5230669964109824, 'subsample': 0.7, 'colsample_bytree': 0.9, 'min_child_weight': 0.2720120994439039, 'reg_alpha': 0.7369542767574853, 'reg_lambda': 0.0029548034837497362, 'max_leaves': 18, 'colsample_bylevel': 0.9217759930378061, 'max_depth': 6}. Best is trial 99 with value: 0.2693426666666667.\n",
      "[I 2023-12-08 21:03:46,245] Trial 101 finished with value: 0.25834466666666667 and parameters: {'n_estimators': 13, 'learning_rate': 0.5656687639795109, 'subsample': 0.7, 'colsample_bytree': 1.0, 'min_child_weight': 0.7896399999740422, 'reg_alpha': 0.2964805622723826, 'reg_lambda': 0.0014628296463969816, 'max_leaves': 20, 'colsample_bylevel': 0.947437753454464, 'max_depth': 6}. Best is trial 101 with value: 0.25834466666666667.\n",
      "[I 2023-12-08 21:03:49,007] Trial 102 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:03:51,812] Trial 103 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:03:56,351] Trial 104 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-08 21:03:59,277] Trial 105 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:04:02,134] Trial 106 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:04:04,888] Trial 107 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:04:10,433] Trial 108 finished with value: 0.276481 and parameters: {'n_estimators': 10, 'learning_rate': 0.5154504555944145, 'subsample': 0.7, 'colsample_bytree': 0.9, 'min_child_weight': 0.6503325775317623, 'reg_alpha': 1.5655609877975167, 'reg_lambda': 0.0021132788689182088, 'max_leaves': 20, 'colsample_bylevel': 0.9837202340062554, 'max_depth': 6}. Best is trial 101 with value: 0.25834466666666667.\n",
      "[I 2023-12-08 21:04:13,198] Trial 109 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:04:17,396] Trial 110 pruned. Trial was pruned at iteration 4.\n",
      "[I 2023-12-08 21:04:20,181] Trial 111 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:04:25,826] Trial 112 finished with value: 0.2772216666666667 and parameters: {'n_estimators': 12, 'learning_rate': 0.5250251058604913, 'subsample': 0.7, 'colsample_bytree': 0.9, 'min_child_weight': 0.5334816365040937, 'reg_alpha': 0.6839225277205087, 'reg_lambda': 0.039081314237887264, 'max_leaves': 19, 'colsample_bylevel': 0.891145428689819, 'max_depth': 6}. Best is trial 101 with value: 0.25834466666666667.\n",
      "[I 2023-12-08 21:04:28,577] Trial 113 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:04:31,261] Trial 114 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:04:34,494] Trial 115 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-08 21:04:37,227] Trial 116 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:04:39,956] Trial 117 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:04:42,797] Trial 118 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:04:45,650] Trial 119 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:04:48,577] Trial 120 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:04:51,430] Trial 121 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:04:54,232] Trial 122 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:04:56,975] Trial 123 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:04:59,887] Trial 124 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:05:02,731] Trial 125 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:05:08,415] Trial 126 finished with value: 0.2806 and parameters: {'n_estimators': 9, 'learning_rate': 0.5365763305379646, 'subsample': 0.7, 'colsample_bytree': 0.7, 'min_child_weight': 0.6385819612709128, 'reg_alpha': 0.463126352165024, 'reg_lambda': 0.0023446640113957613, 'max_leaves': 19, 'colsample_bylevel': 0.8652052036325864, 'max_depth': 6}. Best is trial 101 with value: 0.25834466666666667.\n",
      "[I 2023-12-08 21:05:11,181] Trial 127 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:05:16,812] Trial 128 finished with value: 0.2710706666666666 and parameters: {'n_estimators': 10, 'learning_rate': 0.5530784878644442, 'subsample': 0.6, 'colsample_bytree': 0.9, 'min_child_weight': 0.4132079751014715, 'reg_alpha': 0.9919078713523732, 'reg_lambda': 0.0013653445814356636, 'max_leaves': 19, 'colsample_bylevel': 0.9216127181097007, 'max_depth': 5}. Best is trial 101 with value: 0.25834466666666667.\n",
      "[I 2023-12-08 21:05:22,746] Trial 129 finished with value: 0.28085766666666667 and parameters: {'n_estimators': 11, 'learning_rate': 0.45863452845911246, 'subsample': 0.6, 'colsample_bytree': 0.9, 'min_child_weight': 0.47444551396669915, 'reg_alpha': 0.051534503069033816, 'reg_lambda': 0.0015778257916125027, 'max_leaves': 20, 'colsample_bylevel': 0.9742012718288977, 'max_depth': 5}. Best is trial 101 with value: 0.25834466666666667.\n",
      "[I 2023-12-08 21:05:25,561] Trial 130 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:05:31,156] Trial 131 finished with value: 0.25585366666666665 and parameters: {'n_estimators': 15, 'learning_rate': 0.5986608405669719, 'subsample': 0.7, 'colsample_bytree': 1.0, 'min_child_weight': 0.6420624387578506, 'reg_alpha': 6.691216278741391, 'reg_lambda': 0.0024021031355875664, 'max_leaves': 20, 'colsample_bylevel': 0.9771662244443249, 'max_depth': 6}. Best is trial 131 with value: 0.25585366666666665.\n",
      "[I 2023-12-08 21:05:36,717] Trial 132 finished with value: 0.27206233333333335 and parameters: {'n_estimators': 10, 'learning_rate': 0.5946631173948546, 'subsample': 0.7, 'colsample_bytree': 0.9, 'min_child_weight': 0.6315307777098408, 'reg_alpha': 3.4038512599370887, 'reg_lambda': 0.0016018495922364032, 'max_leaves': 20, 'colsample_bylevel': 0.9341194462239382, 'max_depth': 6}. Best is trial 131 with value: 0.25585366666666665.\n",
      "[I 2023-12-08 21:05:39,537] Trial 133 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:05:42,700] Trial 134 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:05:45,547] Trial 135 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:05:48,873] Trial 136 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:05:52,646] Trial 137 pruned. Trial was pruned at iteration 2.\n",
      "[I 2023-12-08 21:05:55,743] Trial 138 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:05:58,949] Trial 139 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:06:05,289] Trial 140 finished with value: 0.28008533333333335 and parameters: {'n_estimators': 9, 'learning_rate': 0.48193619357297596, 'subsample': 0.7, 'colsample_bytree': 0.9, 'min_child_weight': 0.529585565514149, 'reg_alpha': 1.1771029762764236, 'reg_lambda': 0.002729539775893067, 'max_leaves': 20, 'colsample_bylevel': 0.9255248354949084, 'max_depth': 6}. Best is trial 131 with value: 0.25585366666666665.\n",
      "[I 2023-12-08 21:06:08,765] Trial 141 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:06:11,864] Trial 142 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:06:15,332] Trial 143 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:06:19,701] Trial 144 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:06:23,031] Trial 145 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:06:26,311] Trial 146 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:06:29,800] Trial 147 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:06:37,712] Trial 148 finished with value: 0.28098133333333336 and parameters: {'n_estimators': 11, 'learning_rate': 0.5691311502469902, 'subsample': 0.7, 'colsample_bytree': 0.9, 'min_child_weight': 0.9041038771862211, 'reg_alpha': 2.9982496913905208, 'reg_lambda': 0.001314382839079955, 'max_leaves': 18, 'colsample_bylevel': 0.90498551242668, 'max_depth': 6}. Best is trial 131 with value: 0.25585366666666665.\n",
      "[I 2023-12-08 21:06:41,191] Trial 149 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:06:45,472] Trial 150 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:06:48,927] Trial 151 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:06:52,491] Trial 152 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:06:55,893] Trial 153 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:06:59,536] Trial 154 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:07:02,916] Trial 155 pruned. Trial was pruned at iteration 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning finished! \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\optuna\\samplers\\_tpe\\sampler.py:295: ExperimentalWarning: ``multivariate`` option is an experimental feature. The interface can change in the future.\n",
      "  warnings.warn(\n",
      "[I 2023-12-08 21:07:05,931] A new study created in memory with name: regression\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started tuning xgboost for stator_winding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-08 21:07:12,511] Trial 0 finished with value: 0.29152 and parameters: {'n_estimators': 7, 'learning_rate': 0.43775200014817905, 'subsample': 0.6, 'colsample_bytree': 1.0, 'min_child_weight': 0.8491983767203796, 'reg_alpha': 0.0070689749506246055, 'reg_lambda': 0.005337032762603957, 'max_leaves': 5, 'colsample_bylevel': 0.6521211214797689, 'max_depth': 5}. Best is trial 0 with value: 0.29152.\n",
      "[I 2023-12-08 21:07:19,561] Trial 1 finished with value: 1.0404006666666665 and parameters: {'n_estimators': 8, 'learning_rate': 0.00644285859857343, 'subsample': 0.6, 'colsample_bytree': 0.6, 'min_child_weight': 0.6467903667112945, 'reg_alpha': 0.004809461967501573, 'reg_lambda': 0.0018205657658407262, 'max_leaves': 20, 'colsample_bylevel': 0.9828160165372797, 'max_depth': 6}. Best is trial 0 with value: 0.29152.\n",
      "[I 2023-12-08 21:07:26,302] Trial 2 finished with value: 1.0883033333333334 and parameters: {'n_estimators': 6, 'learning_rate': 0.0018678754137262702, 'subsample': 0.6, 'colsample_bytree': 0.6, 'min_child_weight': 0.5920392514089517, 'reg_alpha': 0.0054880470007660455, 'reg_lambda': 7.556810141274429, 'max_leaves': 16, 'colsample_bylevel': 0.9697494707820946, 'max_depth': 6}. Best is trial 0 with value: 0.29152.\n",
      "[I 2023-12-08 21:07:31,242] Trial 3 finished with value: 0.27440633333333336 and parameters: {'n_estimators': 10, 'learning_rate': 0.3640039183945802, 'subsample': 1.0, 'colsample_bytree': 0.7, 'min_child_weight': 0.2268318024772864, 'reg_alpha': 1.6172900811143154, 'reg_lambda': 0.0019870215385428634, 'max_leaves': 20, 'colsample_bylevel': 0.8861223846483287, 'max_depth': 3}. Best is trial 3 with value: 0.27440633333333336.\n",
      "[I 2023-12-08 21:07:37,263] Trial 4 finished with value: 0.3015156666666667 and parameters: {'n_estimators': 3, 'learning_rate': 0.18427872005083668, 'subsample': 0.8, 'colsample_bytree': 0.7, 'min_child_weight': 0.379884089544096, 'reg_alpha': 0.01998634077852888, 'reg_lambda': 0.8287522363768158, 'max_leaves': 14, 'colsample_bylevel': 0.9436063712881633, 'max_depth': 4}. Best is trial 3 with value: 0.27440633333333336.\n",
      "[I 2023-12-08 21:07:40,838] Trial 5 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:07:44,059] Trial 6 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:07:47,441] Trial 7 pruned. Trial was pruned at iteration 2.\n",
      "[I 2023-12-08 21:07:50,932] Trial 8 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:07:53,595] Trial 9 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:07:56,350] Trial 10 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:07:59,057] Trial 11 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:08:02,001] Trial 12 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:08:06,773] Trial 13 finished with value: 0.282877 and parameters: {'n_estimators': 12, 'learning_rate': 0.29277735186298737, 'subsample': 1.0, 'colsample_bytree': 0.7, 'min_child_weight': 0.5281700409487815, 'reg_alpha': 6.218313275613431, 'reg_lambda': 0.015410751903495607, 'max_leaves': 16, 'colsample_bylevel': 0.919142576935902, 'max_depth': 3}. Best is trial 3 with value: 0.27440633333333336.\n",
      "[I 2023-12-08 21:08:11,842] Trial 14 finished with value: 0.2586173333333333 and parameters: {'n_estimators': 11, 'learning_rate': 0.27794429978492935, 'subsample': 1.0, 'colsample_bytree': 0.7, 'min_child_weight': 0.15723794931536994, 'reg_alpha': 5.032309558387954, 'reg_lambda': 0.019458704650164117, 'max_leaves': 18, 'colsample_bylevel': 0.8570613029859242, 'max_depth': 4}. Best is trial 14 with value: 0.2586173333333333.\n",
      "[I 2023-12-08 21:08:17,001] Trial 15 finished with value: 0.262187 and parameters: {'n_estimators': 10, 'learning_rate': 0.25649692086899134, 'subsample': 1.0, 'colsample_bytree': 0.7, 'min_child_weight': 0.4726061745044262, 'reg_alpha': 0.8103205553552708, 'reg_lambda': 0.0011736494006959503, 'max_leaves': 20, 'colsample_bylevel': 0.9656356941044635, 'max_depth': 4}. Best is trial 14 with value: 0.2586173333333333.\n",
      "[I 2023-12-08 21:08:22,093] Trial 16 finished with value: 0.23658533333333331 and parameters: {'n_estimators': 10, 'learning_rate': 0.4826555144873651, 'subsample': 1.0, 'colsample_bytree': 0.7, 'min_child_weight': 0.4379554325947351, 'reg_alpha': 0.9646763796442414, 'reg_lambda': 0.016707707644236056, 'max_leaves': 20, 'colsample_bylevel': 0.9481855573990278, 'max_depth': 4}. Best is trial 16 with value: 0.23658533333333331.\n",
      "[I 2023-12-08 21:08:24,755] Trial 17 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:08:29,493] Trial 18 finished with value: 0.2736746666666667 and parameters: {'n_estimators': 7, 'learning_rate': 0.37439904506623434, 'subsample': 1.0, 'colsample_bytree': 0.7, 'min_child_weight': 0.6148266455919607, 'reg_alpha': 0.048779713272456164, 'reg_lambda': 0.03957656219221202, 'max_leaves': 20, 'colsample_bylevel': 0.9611265625826507, 'max_depth': 3}. Best is trial 16 with value: 0.23658533333333331.\n",
      "[I 2023-12-08 21:08:32,351] Trial 19 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:08:35,050] Trial 20 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:08:40,101] Trial 21 finished with value: 0.23548533333333332 and parameters: {'n_estimators': 13, 'learning_rate': 0.5530501138300516, 'subsample': 1.0, 'colsample_bytree': 0.8, 'min_child_weight': 0.23993003923379852, 'reg_alpha': 1.7181030118248684, 'reg_lambda': 0.018388575843653745, 'max_leaves': 18, 'colsample_bylevel': 0.8851345968937003, 'max_depth': 4}. Best is trial 21 with value: 0.23548533333333332.\n",
      "[I 2023-12-08 21:08:45,143] Trial 22 finished with value: 0.25226333333333334 and parameters: {'n_estimators': 15, 'learning_rate': 0.3539523899860654, 'subsample': 1.0, 'colsample_bytree': 0.8, 'min_child_weight': 0.1598159029338284, 'reg_alpha': 3.1956955333729993, 'reg_lambda': 0.002118694637870596, 'max_leaves': 14, 'colsample_bylevel': 0.855684819731462, 'max_depth': 4}. Best is trial 21 with value: 0.23548533333333332.\n",
      "[I 2023-12-08 21:08:47,911] Trial 23 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:08:50,710] Trial 24 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:08:53,420] Trial 25 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:08:56,247] Trial 26 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:09:01,374] Trial 27 finished with value: 0.24989733333333333 and parameters: {'n_estimators': 14, 'learning_rate': 0.3431839259198909, 'subsample': 1.0, 'colsample_bytree': 0.8, 'min_child_weight': 0.1564387445396042, 'reg_alpha': 0.6424593770669405, 'reg_lambda': 0.009475468877944936, 'max_leaves': 19, 'colsample_bylevel': 0.8906876787007583, 'max_depth': 4}. Best is trial 21 with value: 0.23548533333333332.\n",
      "[I 2023-12-08 21:09:06,335] Trial 28 finished with value: 0.25466799999999995 and parameters: {'n_estimators': 12, 'learning_rate': 0.42498288059564593, 'subsample': 1.0, 'colsample_bytree': 0.6, 'min_child_weight': 0.15311044164380674, 'reg_alpha': 0.7567348565670136, 'reg_lambda': 0.006804344439077414, 'max_leaves': 16, 'colsample_bylevel': 0.8816090224702916, 'max_depth': 4}. Best is trial 21 with value: 0.23548533333333332.\n",
      "[I 2023-12-08 21:09:11,318] Trial 29 finished with value: 0.234769 and parameters: {'n_estimators': 14, 'learning_rate': 0.5595459042895584, 'subsample': 1.0, 'colsample_bytree': 0.8, 'min_child_weight': 0.1384664503521827, 'reg_alpha': 1.1253673312576873, 'reg_lambda': 0.08111390738636687, 'max_leaves': 17, 'colsample_bylevel': 0.8739367866995802, 'max_depth': 5}. Best is trial 29 with value: 0.234769.\n",
      "[I 2023-12-08 21:09:14,108] Trial 30 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:09:18,964] Trial 31 finished with value: 0.24026533333333333 and parameters: {'n_estimators': 13, 'learning_rate': 0.472689464911339, 'subsample': 1.0, 'colsample_bytree': 0.8, 'min_child_weight': 0.20009347971075492, 'reg_alpha': 1.9227526145910865, 'reg_lambda': 0.7658497311878866, 'max_leaves': 15, 'colsample_bylevel': 0.9910158420491149, 'max_depth': 5}. Best is trial 29 with value: 0.234769.\n",
      "[I 2023-12-08 21:09:21,689] Trial 32 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:09:26,679] Trial 33 finished with value: 0.23158166666666666 and parameters: {'n_estimators': 14, 'learning_rate': 0.5527911175334367, 'subsample': 1.0, 'colsample_bytree': 0.8, 'min_child_weight': 0.29166286700808863, 'reg_alpha': 1.5591220478910162, 'reg_lambda': 0.03751262698106032, 'max_leaves': 19, 'colsample_bylevel': 0.9121466107442645, 'max_depth': 6}. Best is trial 33 with value: 0.23158166666666666.\n",
      "[I 2023-12-08 21:09:29,347] Trial 34 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:09:32,235] Trial 35 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:09:37,179] Trial 36 finished with value: 0.237263 and parameters: {'n_estimators': 13, 'learning_rate': 0.3610308768025685, 'subsample': 1.0, 'colsample_bytree': 0.8, 'min_child_weight': 0.2267375551518743, 'reg_alpha': 1.814070271767273, 'reg_lambda': 0.15720739939409029, 'max_leaves': 20, 'colsample_bylevel': 0.8628831067190555, 'max_depth': 6}. Best is trial 33 with value: 0.23158166666666666.\n",
      "[I 2023-12-08 21:09:41,969] Trial 37 finished with value: 0.24577966666666665 and parameters: {'n_estimators': 15, 'learning_rate': 0.43713444442905175, 'subsample': 1.0, 'colsample_bytree': 1.0, 'min_child_weight': 0.1341780089217204, 'reg_alpha': 1.54236355626639, 'reg_lambda': 0.046463935259979494, 'max_leaves': 12, 'colsample_bylevel': 0.8754572289705328, 'max_depth': 6}. Best is trial 33 with value: 0.23158166666666666.\n",
      "[I 2023-12-08 21:09:47,558] Trial 38 finished with value: 0.23387466666666668 and parameters: {'n_estimators': 15, 'learning_rate': 0.4532876932290879, 'subsample': 0.6, 'colsample_bytree': 0.8, 'min_child_weight': 0.350080546243492, 'reg_alpha': 0.3101435866350964, 'reg_lambda': 0.01258984897806208, 'max_leaves': 19, 'colsample_bylevel': 0.8711903193993165, 'max_depth': 6}. Best is trial 33 with value: 0.23158166666666666.\n",
      "[I 2023-12-08 21:09:53,168] Trial 39 finished with value: 0.22749833333333333 and parameters: {'n_estimators': 12, 'learning_rate': 0.5268421987282844, 'subsample': 0.6, 'colsample_bytree': 0.8, 'min_child_weight': 0.16273899491431015, 'reg_alpha': 0.6510407208744611, 'reg_lambda': 0.005884895879881402, 'max_leaves': 20, 'colsample_bylevel': 0.8214552895978082, 'max_depth': 5}. Best is trial 39 with value: 0.22749833333333333.\n",
      "[I 2023-12-08 21:09:55,902] Trial 40 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:09:58,634] Trial 41 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:10:01,749] Trial 42 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:10:07,528] Trial 43 finished with value: 0.230958 and parameters: {'n_estimators': 15, 'learning_rate': 0.5925340168945282, 'subsample': 0.6, 'colsample_bytree': 0.8, 'min_child_weight': 0.13086107136942313, 'reg_alpha': 2.422780655229384, 'reg_lambda': 0.07612393259216357, 'max_leaves': 18, 'colsample_bylevel': 0.908407937566594, 'max_depth': 5}. Best is trial 39 with value: 0.22749833333333333.\n",
      "[I 2023-12-08 21:10:10,396] Trial 44 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:10:13,123] Trial 45 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:10:15,977] Trial 46 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:10:18,804] Trial 47 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:10:21,683] Trial 48 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:10:24,465] Trial 49 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:10:30,021] Trial 50 finished with value: 0.24396066666666663 and parameters: {'n_estimators': 14, 'learning_rate': 0.4192978956689949, 'subsample': 0.6, 'colsample_bytree': 0.8, 'min_child_weight': 0.12711599246355654, 'reg_alpha': 3.750830138850843, 'reg_lambda': 0.07730124793436471, 'max_leaves': 15, 'colsample_bylevel': 0.8029116210418449, 'max_depth': 4}. Best is trial 39 with value: 0.22749833333333333.\n",
      "[I 2023-12-08 21:10:32,726] Trial 51 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:10:38,440] Trial 52 finished with value: 0.23249499999999998 and parameters: {'n_estimators': 14, 'learning_rate': 0.4374103685608926, 'subsample': 0.6, 'colsample_bytree': 0.8, 'min_child_weight': 0.1869886266208603, 'reg_alpha': 0.7157057211570329, 'reg_lambda': 0.0063572511607905705, 'max_leaves': 20, 'colsample_bylevel': 0.8328019310102566, 'max_depth': 6}. Best is trial 39 with value: 0.22749833333333333.\n",
      "[I 2023-12-08 21:10:44,158] Trial 53 finished with value: 0.22529166666666667 and parameters: {'n_estimators': 13, 'learning_rate': 0.5783683479111265, 'subsample': 0.6, 'colsample_bytree': 0.8, 'min_child_weight': 0.2595390575210585, 'reg_alpha': 2.371803248820765, 'reg_lambda': 0.01432704978853184, 'max_leaves': 19, 'colsample_bylevel': 0.7800167863467408, 'max_depth': 6}. Best is trial 53 with value: 0.22529166666666667.\n",
      "[I 2023-12-08 21:10:49,679] Trial 54 finished with value: 0.233303 and parameters: {'n_estimators': 11, 'learning_rate': 0.5549360836216047, 'subsample': 0.8, 'colsample_bytree': 0.8, 'min_child_weight': 0.37229183809522914, 'reg_alpha': 0.31081161644244076, 'reg_lambda': 0.0036723253455610114, 'max_leaves': 17, 'colsample_bylevel': 0.7908666036779215, 'max_depth': 5}. Best is trial 53 with value: 0.22529166666666667.\n",
      "[I 2023-12-08 21:10:55,149] Trial 55 finished with value: 0.22560666666666665 and parameters: {'n_estimators': 14, 'learning_rate': 0.48213856911351355, 'subsample': 0.6, 'colsample_bytree': 0.8, 'min_child_weight': 0.23115218526420142, 'reg_alpha': 2.48440156242266, 'reg_lambda': 0.40008431275842177, 'max_leaves': 20, 'colsample_bylevel': 0.977136339133161, 'max_depth': 5}. Best is trial 53 with value: 0.22529166666666667.\n",
      "[I 2023-12-08 21:11:00,791] Trial 56 finished with value: 0.225988 and parameters: {'n_estimators': 15, 'learning_rate': 0.5177153190582463, 'subsample': 0.6, 'colsample_bytree': 1.0, 'min_child_weight': 0.37326463159441525, 'reg_alpha': 4.03903888228414, 'reg_lambda': 0.021201914192416994, 'max_leaves': 19, 'colsample_bylevel': 0.7648353146104878, 'max_depth': 6}. Best is trial 53 with value: 0.22529166666666667.\n",
      "[I 2023-12-08 21:11:06,314] Trial 57 finished with value: 0.23322500000000002 and parameters: {'n_estimators': 12, 'learning_rate': 0.42758065185438787, 'subsample': 0.6, 'colsample_bytree': 0.8, 'min_child_weight': 0.19177556742712967, 'reg_alpha': 6.288802781565212, 'reg_lambda': 0.44346118235581083, 'max_leaves': 19, 'colsample_bylevel': 0.9479816706018914, 'max_depth': 5}. Best is trial 53 with value: 0.22529166666666667.\n",
      "[I 2023-12-08 21:11:09,188] Trial 58 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:11:12,161] Trial 59 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:11:14,982] Trial 60 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:11:17,855] Trial 61 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:11:20,595] Trial 62 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:11:23,338] Trial 63 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:11:28,965] Trial 64 finished with value: 0.23452366666666669 and parameters: {'n_estimators': 15, 'learning_rate': 0.5652834331832619, 'subsample': 0.6, 'colsample_bytree': 1.0, 'min_child_weight': 0.42456257784758866, 'reg_alpha': 1.4785256607508195, 'reg_lambda': 0.006664848357100447, 'max_leaves': 15, 'colsample_bylevel': 0.8577050424782509, 'max_depth': 6}. Best is trial 53 with value: 0.22529166666666667.\n",
      "[I 2023-12-08 21:11:34,711] Trial 65 finished with value: 0.23003500000000002 and parameters: {'n_estimators': 14, 'learning_rate': 0.5909509834766798, 'subsample': 0.6, 'colsample_bytree': 0.9, 'min_child_weight': 0.19015275958174505, 'reg_alpha': 6.350745311663443, 'reg_lambda': 0.11163094319425881, 'max_leaves': 16, 'colsample_bylevel': 0.9212091177122133, 'max_depth': 5}. Best is trial 53 with value: 0.22529166666666667.\n",
      "[I 2023-12-08 21:11:37,723] Trial 66 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:11:40,860] Trial 67 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:11:43,700] Trial 68 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:11:46,515] Trial 69 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:11:49,346] Trial 70 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:11:52,315] Trial 71 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:11:57,905] Trial 72 finished with value: 0.233326 and parameters: {'n_estimators': 12, 'learning_rate': 0.48352772704301156, 'subsample': 0.6, 'colsample_bytree': 0.8, 'min_child_weight': 0.12992990302704607, 'reg_alpha': 0.9045873531764874, 'reg_lambda': 0.026943449092501254, 'max_leaves': 18, 'colsample_bylevel': 0.9504568923622696, 'max_depth': 5}. Best is trial 53 with value: 0.22529166666666667.\n",
      "[I 2023-12-08 21:12:00,708] Trial 73 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:12:06,247] Trial 74 finished with value: 0.22965533333333332 and parameters: {'n_estimators': 15, 'learning_rate': 0.4718959420359657, 'subsample': 0.6, 'colsample_bytree': 0.9, 'min_child_weight': 0.13451091010471183, 'reg_alpha': 1.0102970236436246, 'reg_lambda': 0.18845685159080588, 'max_leaves': 18, 'colsample_bylevel': 0.9253658686073014, 'max_depth': 5}. Best is trial 53 with value: 0.22529166666666667.\n",
      "[I 2023-12-08 21:12:09,041] Trial 75 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:12:11,884] Trial 76 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:12:14,836] Trial 77 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:12:17,602] Trial 78 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:12:20,361] Trial 79 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:12:25,838] Trial 80 finished with value: 0.228051 and parameters: {'n_estimators': 12, 'learning_rate': 0.5951348358762762, 'subsample': 0.8, 'colsample_bytree': 0.9, 'min_child_weight': 0.1640541884500394, 'reg_alpha': 2.8879308772335492, 'reg_lambda': 0.3130587678926697, 'max_leaves': 17, 'colsample_bylevel': 0.9086788879741545, 'max_depth': 5}. Best is trial 53 with value: 0.22529166666666667.\n",
      "[I 2023-12-08 21:12:28,588] Trial 81 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:12:34,463] Trial 82 finished with value: 0.23324699999999998 and parameters: {'n_estimators': 15, 'learning_rate': 0.5189399478707925, 'subsample': 0.6, 'colsample_bytree': 0.8, 'min_child_weight': 0.15240533108233198, 'reg_alpha': 9.33502778671565, 'reg_lambda': 0.018952671132773432, 'max_leaves': 18, 'colsample_bylevel': 0.7359758494108068, 'max_depth': 5}. Best is trial 53 with value: 0.22529166666666667.\n",
      "[I 2023-12-08 21:12:37,200] Trial 83 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:12:39,982] Trial 84 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:12:42,689] Trial 85 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:12:45,394] Trial 86 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:12:51,173] Trial 87 finished with value: 0.22770866666666667 and parameters: {'n_estimators': 14, 'learning_rate': 0.5555669211597833, 'subsample': 0.6, 'colsample_bytree': 0.8, 'min_child_weight': 0.20044370595530486, 'reg_alpha': 0.3456415909774264, 'reg_lambda': 0.2677010748469555, 'max_leaves': 19, 'colsample_bylevel': 0.9797210108936415, 'max_depth': 5}. Best is trial 53 with value: 0.22529166666666667.\n",
      "[I 2023-12-08 21:12:53,976] Trial 88 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:12:56,694] Trial 89 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:12:59,466] Trial 90 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:13:02,212] Trial 91 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:13:05,099] Trial 92 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:13:07,861] Trial 93 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:13:10,682] Trial 94 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:13:13,399] Trial 95 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:13:16,360] Trial 96 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:13:19,172] Trial 97 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:13:21,905] Trial 98 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:13:24,644] Trial 99 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:13:27,399] Trial 100 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:13:33,193] Trial 101 finished with value: 0.22500299999999998 and parameters: {'n_estimators': 15, 'learning_rate': 0.5602788524646681, 'subsample': 0.6, 'colsample_bytree': 0.8, 'min_child_weight': 0.23324730110834657, 'reg_alpha': 8.024503471459049, 'reg_lambda': 0.05812052635563366, 'max_leaves': 20, 'colsample_bylevel': 0.989953326616396, 'max_depth': 5}. Best is trial 101 with value: 0.22500299999999998.\n",
      "[I 2023-12-08 21:13:36,498] Trial 102 pruned. Trial was pruned at iteration 2.\n",
      "[I 2023-12-08 21:13:39,238] Trial 103 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:13:41,950] Trial 104 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:13:46,836] Trial 105 finished with value: 0.23048333333333335 and parameters: {'n_estimators': 13, 'learning_rate': 0.506976066121023, 'subsample': 1.0, 'colsample_bytree': 0.8, 'min_child_weight': 0.3254063302820862, 'reg_alpha': 0.4894394231403978, 'reg_lambda': 0.0402445866634232, 'max_leaves': 19, 'colsample_bylevel': 0.8793260694980811, 'max_depth': 6}. Best is trial 101 with value: 0.22500299999999998.\n",
      "[I 2023-12-08 21:13:52,590] Trial 106 finished with value: 0.22845233333333334 and parameters: {'n_estimators': 14, 'learning_rate': 0.5490344054545946, 'subsample': 0.6, 'colsample_bytree': 0.8, 'min_child_weight': 0.2460222794519144, 'reg_alpha': 3.17391726486662, 'reg_lambda': 0.3892365721735781, 'max_leaves': 20, 'colsample_bylevel': 0.9465707275405828, 'max_depth': 5}. Best is trial 101 with value: 0.22500299999999998.\n",
      "[I 2023-12-08 21:13:55,381] Trial 107 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:13:58,095] Trial 108 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:14:01,030] Trial 109 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:14:06,598] Trial 110 finished with value: 0.22710033333333335 and parameters: {'n_estimators': 15, 'learning_rate': 0.4917996963569948, 'subsample': 0.6, 'colsample_bytree': 1.0, 'min_child_weight': 0.4959501641842614, 'reg_alpha': 0.34649051285004123, 'reg_lambda': 0.012915027482346615, 'max_leaves': 20, 'colsample_bylevel': 0.7943030212678935, 'max_depth': 6}. Best is trial 101 with value: 0.22500299999999998.\n",
      "[I 2023-12-08 21:14:09,542] Trial 111 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:14:15,358] Trial 112 finished with value: 0.2249036666666667 and parameters: {'n_estimators': 15, 'learning_rate': 0.5225519136284105, 'subsample': 0.6, 'colsample_bytree': 1.0, 'min_child_weight': 0.38184169848045957, 'reg_alpha': 0.17513281093473634, 'reg_lambda': 0.19015839999609446, 'max_leaves': 19, 'colsample_bylevel': 0.7328581303537035, 'max_depth': 6}. Best is trial 112 with value: 0.2249036666666667.\n",
      "[I 2023-12-08 21:14:18,117] Trial 113 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:14:20,828] Trial 114 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:14:24,193] Trial 115 pruned. Trial was pruned at iteration 2.\n",
      "[I 2023-12-08 21:14:29,685] Trial 116 finished with value: 0.231505 and parameters: {'n_estimators': 15, 'learning_rate': 0.5610215977628109, 'subsample': 0.8, 'colsample_bytree': 0.8, 'min_child_weight': 0.38395809800647535, 'reg_alpha': 9.694796444412264, 'reg_lambda': 0.11220599360398774, 'max_leaves': 18, 'colsample_bylevel': 0.9828951155514783, 'max_depth': 5}. Best is trial 112 with value: 0.2249036666666667.\n",
      "[I 2023-12-08 21:14:32,514] Trial 117 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:14:35,247] Trial 118 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:14:38,029] Trial 119 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:14:43,500] Trial 120 finished with value: 0.230164 and parameters: {'n_estimators': 15, 'learning_rate': 0.5813826198428701, 'subsample': 0.6, 'colsample_bytree': 1.0, 'min_child_weight': 0.439871289155869, 'reg_alpha': 2.382527744246831, 'reg_lambda': 0.041644436374594036, 'max_leaves': 16, 'colsample_bylevel': 0.7578825853748554, 'max_depth': 6}. Best is trial 112 with value: 0.2249036666666667.\n",
      "[I 2023-12-08 21:14:46,227] Trial 121 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:14:49,008] Trial 122 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:14:51,956] Trial 123 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:14:55,021] Trial 124 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:14:57,781] Trial 125 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:15:00,618] Trial 126 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:15:03,647] Trial 127 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:15:06,428] Trial 128 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:15:09,192] Trial 129 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:15:12,067] Trial 130 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:15:17,860] Trial 131 finished with value: 0.22297966666666666 and parameters: {'n_estimators': 15, 'learning_rate': 0.5986427251000744, 'subsample': 0.6, 'colsample_bytree': 0.8, 'min_child_weight': 0.3430566427875238, 'reg_alpha': 9.30348117067426, 'reg_lambda': 0.04246650170054556, 'max_leaves': 20, 'colsample_bylevel': 0.983960402745943, 'max_depth': 5}. Best is trial 131 with value: 0.22297966666666666.\n",
      "[I 2023-12-08 21:15:20,845] Trial 132 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:15:23,582] Trial 133 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:15:26,337] Trial 134 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:15:29,109] Trial 135 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:15:32,066] Trial 136 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:15:34,821] Trial 137 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:15:37,542] Trial 138 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:15:40,244] Trial 139 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:15:43,161] Trial 140 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:15:45,870] Trial 141 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:15:51,574] Trial 142 finished with value: 0.22815666666666665 and parameters: {'n_estimators': 13, 'learning_rate': 0.49787831964220286, 'subsample': 0.6, 'colsample_bytree': 0.8, 'min_child_weight': 0.13673492827294634, 'reg_alpha': 8.227436820580927, 'reg_lambda': 0.007121961719514944, 'max_leaves': 20, 'colsample_bylevel': 0.9691451756754056, 'max_depth': 5}. Best is trial 131 with value: 0.22297966666666666.\n",
      "[I 2023-12-08 21:15:54,373] Trial 143 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:15:57,085] Trial 144 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:15:59,854] Trial 145 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:16:02,800] Trial 146 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:16:05,489] Trial 147 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:16:08,598] Trial 148 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-08 21:16:11,436] Trial 149 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:16:14,397] Trial 150 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:16:17,115] Trial 151 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:16:19,772] Trial 152 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:16:22,506] Trial 153 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:16:25,547] Trial 154 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:16:28,277] Trial 155 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:16:31,071] Trial 156 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:16:33,874] Trial 157 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:16:39,569] Trial 158 finished with value: 0.227016 and parameters: {'n_estimators': 14, 'learning_rate': 0.5817294108984645, 'subsample': 0.6, 'colsample_bytree': 0.8, 'min_child_weight': 0.38736627712502664, 'reg_alpha': 9.54745527404722, 'reg_lambda': 0.07680056116782195, 'max_leaves': 19, 'colsample_bylevel': 0.9491916076542717, 'max_depth': 5}. Best is trial 131 with value: 0.22297966666666666.\n",
      "[I 2023-12-08 21:16:42,463] Trial 159 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:16:45,454] Trial 160 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-08 21:16:48,149] Trial 161 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:16:53,845] Trial 162 finished with value: 0.22836033333333336 and parameters: {'n_estimators': 15, 'learning_rate': 0.5665570231650852, 'subsample': 0.6, 'colsample_bytree': 0.8, 'min_child_weight': 0.2974592928926105, 'reg_alpha': 9.441393519386951, 'reg_lambda': 3.5092550567992156, 'max_leaves': 19, 'colsample_bylevel': 0.9729067198501593, 'max_depth': 5}. Best is trial 131 with value: 0.22297966666666666.\n",
      "[I 2023-12-08 21:16:56,615] Trial 163 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:16:59,431] Trial 164 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-08 21:17:05,240] Trial 165 finished with value: 0.22899766666666665 and parameters: {'n_estimators': 15, 'learning_rate': 0.5407752685807435, 'subsample': 0.6, 'colsample_bytree': 0.8, 'min_child_weight': 0.31698412494598455, 'reg_alpha': 2.8679113176372963, 'reg_lambda': 1.167281648378938, 'max_leaves': 20, 'colsample_bylevel': 0.9585125896772364, 'max_depth': 6}. Best is trial 131 with value: 0.22297966666666666.\n",
      "[I 2023-12-08 21:17:08,000] Trial 166 pruned. Trial was pruned at iteration 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning finished! \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/12/08 21:17:10 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n"
     ]
    }
   ],
   "source": [
    "mlflow_run_name =  \"XGBoost\" \n",
    "model_name = \"xgboost\"\n",
    "direction = \"minimize\"\n",
    "task = \"regression\"\n",
    "# time allocated for each target in seconds\n",
    "timeout = 10*60 \n",
    "mlflow_model_name = \"xgboost\"\n",
    "n_splits = 3\n",
    "# to be specified for LGBM and XGBoost\n",
    "model_objective = \"reg:squarederror\" \n",
    "# base score function in cross validation\n",
    "base_score_function = mean_squared_error\n",
    "# LGBM and XGBoost cross validation score function\n",
    "def cv_score_function(y_true, pred):\n",
    "    \"\"\"Returns score\n",
    "\n",
    "    Args:\n",
    "        y_true (Series): ground truth data\n",
    "        y_pred (DMatrix): DMatrix XGBoost object\n",
    "    \n",
    "    \"\"\"\n",
    "    return(base_score_function.__name__, base_score_function(y_true, pred.get_label()))\n",
    "cv_score_function.__name__ = base_score_function.__name__\n",
    "targets = num_targets\n",
    "# calling main function\n",
    "score_dict, y_model_dict, opt_model_dict = tune_model(\n",
    "            run_name=mlflow_run_name, \n",
    "            model_name=model_name,\n",
    "            direction=direction,\n",
    "            task=task,\n",
    "            timeout=timeout,\n",
    "            targets=targets,\n",
    "            X_train=X_train, \n",
    "            y_train=y_train, \n",
    "            X_test=X_test, \n",
    "            y_test=y_test,\n",
    "            n_splits=n_splits,\n",
    "            random_state=random_state, \n",
    "            model_objective=model_objective,\n",
    "            base_score_function=base_score_function,\n",
    "            cv_score_function=cv_score_function,\n",
    "            mlflow_model_name=mlflow_model_name,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pm': {'xgboost': {'RMSE': {'train': 0.46967198531308074,\n",
       "    'test': 0.4695499996884248},\n",
       "   'MAE': {'train': 0.3505366642714987, 'test': 0.3507050327158062},\n",
       "   'R2': {'train': 0.8010777730194634, 'test': 0.8009848659294391}}},\n",
       " 'stator_winding': {'xgboost': {'RMSE': {'train': 0.45203377958038365,\n",
       "    'test': 0.45343238299089145},\n",
       "   'MAE': {'train': 0.32043690223923843, 'test': 0.32220857906207134},\n",
       "   'R2': {'train': 0.8160442929427676, 'test': 0.8149035860358}}}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\optuna\\samplers\\_tpe\\sampler.py:295: ExperimentalWarning: ``multivariate`` option is an experimental feature. The interface can change in the future.\n",
      "  warnings.warn(\n",
      "[I 2023-12-08 21:17:22,876] A new study created in memory with name: classification\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started tuning lgbm for stator_yoke...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\lightgbm\\engine.py:685: UserWarning: Found 'n_estimators' in params. Will use it instead of 'num_boost_round' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'num_boost_round' argument\")\n",
      "[I 2023-12-08 21:17:33,022] Trial 0 finished with value: 0.0 and parameters: {'n_estimators': 13, 'reg_alpha': 6.351221010640703, 'reg_lambda': 0.8471801418819978, 'colsample_bytree': 0.7, 'subsample': 0.4, 'learning_rate': 0.014648955132800727, 'max_depth': 4, 'num_leaves': 15, 'min_child_samples': 5}. Best is trial 0 with value: 0.0.\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\lightgbm\\engine.py:685: UserWarning: Found 'n_estimators' in params. Will use it instead of 'num_boost_round' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'num_boost_round' argument\")\n",
      "[I 2023-12-08 21:17:40,657] Trial 1 finished with value: 0.0 and parameters: {'n_estimators': 11, 'reg_alpha': 0.029204338471814112, 'reg_lambda': 0.06672367170464207, 'colsample_bytree': 0.3, 'subsample': 0.5, 'learning_rate': 0.0021348999901951954, 'max_depth': 4, 'num_leaves': 6, 'min_child_samples': 19}. Best is trial 0 with value: 0.0.\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\lightgbm\\engine.py:685: UserWarning: Found 'n_estimators' in params. Will use it instead of 'num_boost_round' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'num_boost_round' argument\")\n",
      "[I 2023-12-08 21:17:48,361] Trial 2 finished with value: 0.0 and parameters: {'n_estimators': 10, 'reg_alpha': 0.4467752817973907, 'reg_lambda': 0.017654048052495083, 'colsample_bytree': 0.6, 'subsample': 0.4, 'learning_rate': 0.17247957710046016, 'max_depth': 4, 'num_leaves': 10, 'min_child_samples': 12}. Best is trial 0 with value: 0.0.\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\lightgbm\\engine.py:685: UserWarning: Found 'n_estimators' in params. Will use it instead of 'num_boost_round' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'num_boost_round' argument\")\n",
      "[I 2023-12-08 21:17:53,106] Trial 3 finished with value: 0.0 and parameters: {'n_estimators': 6, 'reg_alpha': 1.6172900811143154, 'reg_lambda': 0.0019870215385428634, 'colsample_bytree': 0.3, 'subsample': 0.7, 'learning_rate': 0.00690767598589619, 'max_depth': 4, 'num_leaves': 16, 'min_child_samples': 14}. Best is trial 0 with value: 0.0.\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\lightgbm\\engine.py:685: UserWarning: Found 'n_estimators' in params. Will use it instead of 'num_boost_round' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'num_boost_round' argument\")\n",
      "[I 2023-12-08 21:18:09,937] Trial 4 finished with value: 0.0 and parameters: {'n_estimators': 27, 'reg_alpha': 0.0774211647399625, 'reg_lambda': 0.003008686821445846, 'colsample_bytree': 0.6, 'subsample': 0.9, 'learning_rate': 0.012811830949395147, 'max_depth': 6, 'num_leaves': 9, 'min_child_samples': 4}. Best is trial 0 with value: 0.0.\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\lightgbm\\engine.py:685: UserWarning: Found 'n_estimators' in params. Will use it instead of 'num_boost_round' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'num_boost_round' argument\")\n",
      "[I 2023-12-08 21:18:17,579] Trial 5 finished with value: 0.0 and parameters: {'n_estimators': 11, 'reg_alpha': 0.00441453687649448, 'reg_lambda': 5.233480488540088, 'colsample_bytree': 0.8, 'subsample': 0.4, 'learning_rate': 0.0010441516804561306, 'max_depth': 5, 'num_leaves': 12, 'min_child_samples': 6}. Best is trial 0 with value: 0.0.\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\lightgbm\\engine.py:685: UserWarning: Found 'n_estimators' in params. Will use it instead of 'num_boost_round' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'num_boost_round' argument\")\n",
      "[I 2023-12-08 21:18:22,239] Trial 6 finished with value: 0.0 and parameters: {'n_estimators': 6, 'reg_alpha': 0.022410971619109515, 'reg_lambda': 5.910698619088545, 'colsample_bytree': 0.7, 'subsample': 0.7, 'learning_rate': 0.2827368285260592, 'max_depth': 3, 'num_leaves': 8, 'min_child_samples': 11}. Best is trial 0 with value: 0.0.\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\lightgbm\\engine.py:685: UserWarning: Found 'n_estimators' in params. Will use it instead of 'num_boost_round' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'num_boost_round' argument\")\n",
      "[I 2023-12-08 21:18:40,458] Trial 7 finished with value: 0.0 and parameters: {'n_estimators': 30, 'reg_alpha': 0.009294394155644996, 'reg_lambda': 0.4881375191603672, 'colsample_bytree': 0.3, 'subsample': 0.4, 'learning_rate': 0.024105925153067684, 'max_depth': 3, 'num_leaves': 15, 'min_child_samples': 6}. Best is trial 0 with value: 0.0.\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\lightgbm\\engine.py:685: UserWarning: Found 'n_estimators' in params. Will use it instead of 'num_boost_round' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'num_boost_round' argument\")\n",
      "[I 2023-12-08 21:18:54,621] Trial 8 finished with value: 0.0 and parameters: {'n_estimators': 22, 'reg_alpha': 0.03523233163198313, 'reg_lambda': 5.583672722754823, 'colsample_bytree': 0.6, 'subsample': 0.9, 'learning_rate': 0.008223017918502924, 'max_depth': 4, 'num_leaves': 16, 'min_child_samples': 19}. Best is trial 0 with value: 0.0.\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\lightgbm\\engine.py:685: UserWarning: Found 'n_estimators' in params. Will use it instead of 'num_boost_round' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'num_boost_round' argument\")\n",
      "[I 2023-12-08 21:19:12,038] Trial 9 finished with value: 0.0 and parameters: {'n_estimators': 27, 'reg_alpha': 1.3167465326936145, 'reg_lambda': 0.3699359899959751, 'colsample_bytree': 0.5, 'subsample': 0.9, 'learning_rate': 0.007555166239631481, 'max_depth': 5, 'num_leaves': 15, 'min_child_samples': 18}. Best is trial 0 with value: 0.0.\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\lightgbm\\engine.py:685: UserWarning: Found 'n_estimators' in params. Will use it instead of 'num_boost_round' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'num_boost_round' argument\")\n",
      "[I 2023-12-08 21:19:25,795] Trial 10 finished with value: 0.0 and parameters: {'n_estimators': 21, 'reg_alpha': 2.1352741637326655, 'reg_lambda': 4.304975897124975, 'colsample_bytree': 0.7, 'subsample': 0.4, 'learning_rate': 0.026226305890658212, 'max_depth': 4, 'num_leaves': 19, 'min_child_samples': 6}. Best is trial 0 with value: 0.0.\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\lightgbm\\engine.py:685: UserWarning: Found 'n_estimators' in params. Will use it instead of 'num_boost_round' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'num_boost_round' argument\")\n",
      "[I 2023-12-08 21:19:30,604] Trial 11 finished with value: 0.0 and parameters: {'n_estimators': 6, 'reg_alpha': 3.315122605144113, 'reg_lambda': 0.292785056989379, 'colsample_bytree': 0.7, 'subsample': 0.4, 'learning_rate': 0.02468828479288294, 'max_depth': 4, 'num_leaves': 13, 'min_child_samples': 8}. Best is trial 0 with value: 0.0.\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\lightgbm\\engine.py:685: UserWarning: Found 'n_estimators' in params. Will use it instead of 'num_boost_round' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'num_boost_round' argument\")\n",
      "[I 2023-12-08 21:19:34,186] Trial 12 finished with value: 0.0 and parameters: {'n_estimators': 4, 'reg_alpha': 0.07918434155029005, 'reg_lambda': 0.009341922814407702, 'colsample_bytree': 0.3, 'subsample': 0.5, 'learning_rate': 0.0034635786322727378, 'max_depth': 3, 'num_leaves': 6, 'min_child_samples': 12}. Best is trial 0 with value: 0.0.\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\lightgbm\\engine.py:685: UserWarning: Found 'n_estimators' in params. Will use it instead of 'num_boost_round' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'num_boost_round' argument\")\n",
      "[I 2023-12-08 21:19:42,740] Trial 13 finished with value: 0.0 and parameters: {'n_estimators': 13, 'reg_alpha': 0.25318675103624155, 'reg_lambda': 0.007188016769522182, 'colsample_bytree': 0.3, 'subsample': 0.5, 'learning_rate': 0.005125967430186091, 'max_depth': 6, 'num_leaves': 6, 'min_child_samples': 17}. Best is trial 0 with value: 0.0.\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\lightgbm\\engine.py:685: UserWarning: Found 'n_estimators' in params. Will use it instead of 'num_boost_round' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'num_boost_round' argument\")\n",
      "[I 2023-12-08 21:19:50,384] Trial 14 finished with value: 0.0 and parameters: {'n_estimators': 11, 'reg_alpha': 0.007354522723162921, 'reg_lambda': 0.9113907384447003, 'colsample_bytree': 0.9, 'subsample': 0.5, 'learning_rate': 0.0011238795349329228, 'max_depth': 5, 'num_leaves': 6, 'min_child_samples': 20}. Best is trial 0 with value: 0.0.\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\lightgbm\\engine.py:685: UserWarning: Found 'n_estimators' in params. Will use it instead of 'num_boost_round' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'num_boost_round' argument\")\n",
      "[I 2023-12-08 21:20:01,386] Trial 15 finished with value: 0.0 and parameters: {'n_estimators': 17, 'reg_alpha': 0.41320708495157055, 'reg_lambda': 0.23276595924851196, 'colsample_bytree': 0.3, 'subsample': 0.5, 'learning_rate': 0.0032732026947565854, 'max_depth': 4, 'num_leaves': 9, 'min_child_samples': 17}. Best is trial 0 with value: 0.0.\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\lightgbm\\engine.py:685: UserWarning: Found 'n_estimators' in params. Will use it instead of 'num_boost_round' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'num_boost_round' argument\")\n",
      "[I 2023-12-08 21:20:07,164] Trial 16 finished with value: 0.0 and parameters: {'n_estimators': 8, 'reg_alpha': 0.002975578251866914, 'reg_lambda': 0.017956152806390225, 'colsample_bytree': 0.3, 'subsample': 0.4, 'learning_rate': 0.005541873285251221, 'max_depth': 5, 'num_leaves': 6, 'min_child_samples': 18}. Best is trial 0 with value: 0.0.\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\lightgbm\\engine.py:685: UserWarning: Found 'n_estimators' in params. Will use it instead of 'num_boost_round' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'num_boost_round' argument\")\n",
      "[I 2023-12-08 21:20:17,271] Trial 17 finished with value: 0.0 and parameters: {'n_estimators': 15, 'reg_alpha': 5.428089866173941, 'reg_lambda': 0.7422477791332904, 'colsample_bytree': 0.7, 'subsample': 0.4, 'learning_rate': 0.017713655735232568, 'max_depth': 5, 'num_leaves': 11, 'min_child_samples': 3}. Best is trial 0 with value: 0.0.\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\lightgbm\\engine.py:685: UserWarning: Found 'n_estimators' in params. Will use it instead of 'num_boost_round' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'num_boost_round' argument\")\n",
      "[I 2023-12-08 21:20:26,686] Trial 18 finished with value: 0.0 and parameters: {'n_estimators': 14, 'reg_alpha': 0.019624901430659883, 'reg_lambda': 0.05264760738151218, 'colsample_bytree': 0.6, 'subsample': 0.9, 'learning_rate': 0.001705501356836509, 'max_depth': 4, 'num_leaves': 7, 'min_child_samples': 20}. Best is trial 0 with value: 0.0.\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\lightgbm\\engine.py:685: UserWarning: Found 'n_estimators' in params. Will use it instead of 'num_boost_round' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'num_boost_round' argument\")\n",
      "[I 2023-12-08 21:20:30,199] Trial 19 finished with value: 0.0 and parameters: {'n_estimators': 4, 'reg_alpha': 0.09056761734485544, 'reg_lambda': 0.26908572952203136, 'colsample_bytree': 0.3, 'subsample': 0.5, 'learning_rate': 0.0036464743482264093, 'max_depth': 3, 'num_leaves': 10, 'min_child_samples': 18}. Best is trial 0 with value: 0.0.\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\lightgbm\\engine.py:685: UserWarning: Found 'n_estimators' in params. Will use it instead of 'num_boost_round' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'num_boost_round' argument\")\n",
      "[I 2023-12-08 21:20:41,980] Trial 20 finished with value: 0.0 and parameters: {'n_estimators': 18, 'reg_alpha': 3.823617550587795, 'reg_lambda': 1.8898031132543291, 'colsample_bytree': 0.6, 'subsample': 0.8, 'learning_rate': 0.018229072644163843, 'max_depth': 4, 'num_leaves': 13, 'min_child_samples': 6}. Best is trial 0 with value: 0.0.\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\lightgbm\\engine.py:685: UserWarning: Found 'n_estimators' in params. Will use it instead of 'num_boost_round' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'num_boost_round' argument\")\n",
      "[I 2023-12-08 21:20:49,040] Trial 21 finished with value: 0.0 and parameters: {'n_estimators': 10, 'reg_alpha': 0.7526593137348659, 'reg_lambda': 0.24726657821272963, 'colsample_bytree': 0.6, 'subsample': 0.4, 'learning_rate': 0.16132197330225365, 'max_depth': 4, 'num_leaves': 8, 'min_child_samples': 7}. Best is trial 0 with value: 0.0.\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\lightgbm\\engine.py:685: UserWarning: Found 'n_estimators' in params. Will use it instead of 'num_boost_round' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'num_boost_round' argument\")\n",
      "[I 2023-12-08 21:20:58,370] Trial 22 finished with value: 0.0 and parameters: {'n_estimators': 14, 'reg_alpha': 0.7804591286543245, 'reg_lambda': 0.02919364064463365, 'colsample_bytree': 0.6, 'subsample': 0.7, 'learning_rate': 0.24431476350710118, 'max_depth': 5, 'num_leaves': 8, 'min_child_samples': 12}. Best is trial 0 with value: 0.0.\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\lightgbm\\engine.py:685: UserWarning: Found 'n_estimators' in params. Will use it instead of 'num_boost_round' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'num_boost_round' argument\")\n",
      "[I 2023-12-08 21:21:04,767] Trial 23 finished with value: 0.0 and parameters: {'n_estimators': 9, 'reg_alpha': 0.22608302311873144, 'reg_lambda': 0.03465862864287987, 'colsample_bytree': 0.6, 'subsample': 0.4, 'learning_rate': 0.08575264582066401, 'max_depth': 3, 'num_leaves': 13, 'min_child_samples': 11}. Best is trial 0 with value: 0.0.\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\lightgbm\\engine.py:685: UserWarning: Found 'n_estimators' in params. Will use it instead of 'num_boost_round' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'num_boost_round' argument\")\n",
      "[I 2023-12-08 21:21:11,253] Trial 24 finished with value: 0.0 and parameters: {'n_estimators': 9, 'reg_alpha': 0.01269575613877386, 'reg_lambda': 0.414111835088724, 'colsample_bytree': 0.7, 'subsample': 0.5, 'learning_rate': 0.010585230838458342, 'max_depth': 4, 'num_leaves': 6, 'min_child_samples': 20}. Best is trial 0 with value: 0.0.\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\lightgbm\\engine.py:685: UserWarning: Found 'n_estimators' in params. Will use it instead of 'num_boost_round' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'num_boost_round' argument\")\n",
      "[I 2023-12-08 21:21:22,831] Trial 25 finished with value: 0.0 and parameters: {'n_estimators': 18, 'reg_alpha': 9.464364175682105, 'reg_lambda': 1.1119710302326509, 'colsample_bytree': 0.4, 'subsample': 0.4, 'learning_rate': 0.012789240635225161, 'max_depth': 3, 'num_leaves': 13, 'min_child_samples': 3}. Best is trial 0 with value: 0.0.\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\lightgbm\\engine.py:685: UserWarning: Found 'n_estimators' in params. Will use it instead of 'num_boost_round' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'num_boost_round' argument\")\n",
      "[I 2023-12-08 21:21:36,755] Trial 26 finished with value: 0.0 and parameters: {'n_estimators': 22, 'reg_alpha': 0.012376971636104336, 'reg_lambda': 0.058670056314012106, 'colsample_bytree': 0.3, 'subsample': 0.5, 'learning_rate': 0.003623526983183697, 'max_depth': 5, 'num_leaves': 8, 'min_child_samples': 18}. Best is trial 0 with value: 0.0.\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\lightgbm\\engine.py:685: UserWarning: Found 'n_estimators' in params. Will use it instead of 'num_boost_round' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'num_boost_round' argument\")\n",
      "[I 2023-12-08 21:21:47,285] Trial 27 finished with value: 0.0 and parameters: {'n_estimators': 16, 'reg_alpha': 0.5844896508702799, 'reg_lambda': 0.003793342466221994, 'colsample_bytree': 0.6, 'subsample': 0.4, 'learning_rate': 0.07108706922202782, 'max_depth': 4, 'num_leaves': 11, 'min_child_samples': 13}. Best is trial 0 with value: 0.0.\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\lightgbm\\engine.py:685: UserWarning: Found 'n_estimators' in params. Will use it instead of 'num_boost_round' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'num_boost_round' argument\")\n",
      "[I 2023-12-08 21:21:54,250] Trial 28 finished with value: 0.0 and parameters: {'n_estimators': 10, 'reg_alpha': 0.0461925987302171, 'reg_lambda': 0.019774391821503003, 'colsample_bytree': 0.3, 'subsample': 0.7, 'learning_rate': 0.0012887152781088783, 'max_depth': 4, 'num_leaves': 6, 'min_child_samples': 16}. Best is trial 0 with value: 0.0.\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\lightgbm\\engine.py:685: UserWarning: Found 'n_estimators' in params. Will use it instead of 'num_boost_round' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'num_boost_round' argument\")\n",
      "[I 2023-12-08 21:22:02,313] Trial 29 finished with value: 0.0 and parameters: {'n_estimators': 11, 'reg_alpha': 4.574556666210157, 'reg_lambda': 0.8087024515698671, 'colsample_bytree': 0.7, 'subsample': 0.5, 'learning_rate': 0.05532521925802402, 'max_depth': 4, 'num_leaves': 16, 'min_child_samples': 7}. Best is trial 0 with value: 0.0.\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\lightgbm\\engine.py:685: UserWarning: Found 'n_estimators' in params. Will use it instead of 'num_boost_round' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'num_boost_round' argument\")\n",
      "[I 2023-12-08 21:22:10,065] Trial 30 finished with value: 0.0 and parameters: {'n_estimators': 11, 'reg_alpha': 0.11878796621020107, 'reg_lambda': 0.1274738992170916, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.008393658154846157, 'max_depth': 3, 'num_leaves': 6, 'min_child_samples': 17}. Best is trial 0 with value: 0.0.\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\lightgbm\\engine.py:685: UserWarning: Found 'n_estimators' in params. Will use it instead of 'num_boost_round' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'num_boost_round' argument\")\n",
      "[I 2023-12-08 21:22:18,300] Trial 31 finished with value: 0.0 and parameters: {'n_estimators': 11, 'reg_alpha': 8.73115629309056, 'reg_lambda': 0.7489454465864213, 'colsample_bytree': 0.7, 'subsample': 0.4, 'learning_rate': 0.0021850216577961276, 'max_depth': 4, 'num_leaves': 16, 'min_child_samples': 7}. Best is trial 0 with value: 0.0.\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\lightgbm\\engine.py:685: UserWarning: Found 'n_estimators' in params. Will use it instead of 'num_boost_round' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'num_boost_round' argument\")\n",
      "[I 2023-12-08 21:22:23,284] Trial 32 finished with value: 0.0 and parameters: {'n_estimators': 6, 'reg_alpha': 1.2455167016964712, 'reg_lambda': 0.027564345118283665, 'colsample_bytree': 0.3, 'subsample': 0.7, 'learning_rate': 0.008104139689607626, 'max_depth': 3, 'num_leaves': 14, 'min_child_samples': 15}. Best is trial 0 with value: 0.0.\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\lightgbm\\engine.py:685: UserWarning: Found 'n_estimators' in params. Will use it instead of 'num_boost_round' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'num_boost_round' argument\")\n",
      "[I 2023-12-08 21:22:26,961] Trial 33 finished with value: 0.0 and parameters: {'n_estimators': 4, 'reg_alpha': 5.940499320052548, 'reg_lambda': 0.0046067229197188555, 'colsample_bytree': 0.3, 'subsample': 0.7, 'learning_rate': 0.010820493981258152, 'max_depth': 4, 'num_leaves': 18, 'min_child_samples': 17}. Best is trial 0 with value: 0.0.\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\lightgbm\\engine.py:685: UserWarning: Found 'n_estimators' in params. Will use it instead of 'num_boost_round' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'num_boost_round' argument\")\n",
      "[I 2023-12-08 21:22:37,293] Trial 34 finished with value: 0.0 and parameters: {'n_estimators': 15, 'reg_alpha': 0.06672834049056219, 'reg_lambda': 0.04447093227594826, 'colsample_bytree': 1.0, 'subsample': 0.5, 'learning_rate': 0.0013428066984126428, 'max_depth': 4, 'num_leaves': 9, 'min_child_samples': 19}. Best is trial 0 with value: 0.0.\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\lightgbm\\engine.py:685: UserWarning: Found 'n_estimators' in params. Will use it instead of 'num_boost_round' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'num_boost_round' argument\")\n",
      "[I 2023-12-08 21:22:48,671] Trial 35 finished with value: 0.0 and parameters: {'n_estimators': 17, 'reg_alpha': 0.3035804168091339, 'reg_lambda': 0.24509171171056893, 'colsample_bytree': 0.7, 'subsample': 0.8, 'learning_rate': 0.016344358514154257, 'max_depth': 4, 'num_leaves': 16, 'min_child_samples': 5}. Best is trial 0 with value: 0.0.\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\lightgbm\\engine.py:685: UserWarning: Found 'n_estimators' in params. Will use it instead of 'num_boost_round' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'num_boost_round' argument\")\n",
      "[I 2023-12-08 21:22:54,711] Trial 36 finished with value: 0.0 and parameters: {'n_estimators': 8, 'reg_alpha': 0.8256660623220853, 'reg_lambda': 0.0013925897280211588, 'colsample_bytree': 0.6, 'subsample': 0.7, 'learning_rate': 0.004466376675768805, 'max_depth': 4, 'num_leaves': 16, 'min_child_samples': 15}. Best is trial 0 with value: 0.0.\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\lightgbm\\engine.py:685: UserWarning: Found 'n_estimators' in params. Will use it instead of 'num_boost_round' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'num_boost_round' argument\")\n",
      "[I 2023-12-08 21:23:04,339] Trial 37 finished with value: 0.0 and parameters: {'n_estimators': 14, 'reg_alpha': 6.120839096991753, 'reg_lambda': 1.9300341801141812, 'colsample_bytree': 0.8, 'subsample': 0.4, 'learning_rate': 0.01572565272502592, 'max_depth': 4, 'num_leaves': 18, 'min_child_samples': 6}. Best is trial 0 with value: 0.0.\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\lightgbm\\engine.py:685: UserWarning: Found 'n_estimators' in params. Will use it instead of 'num_boost_round' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'num_boost_round' argument\")\n",
      "[I 2023-12-08 21:23:08,544] Trial 38 finished with value: 0.0 and parameters: {'n_estimators': 5, 'reg_alpha': 2.966038845995985, 'reg_lambda': 0.004566295169830686, 'colsample_bytree': 0.3, 'subsample': 0.4, 'learning_rate': 0.3327490197562127, 'max_depth': 4, 'num_leaves': 8, 'min_child_samples': 14}. Best is trial 0 with value: 0.0.\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\lightgbm\\engine.py:685: UserWarning: Found 'n_estimators' in params. Will use it instead of 'num_boost_round' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'num_boost_round' argument\")\n",
      "[I 2023-12-08 21:23:13,867] Trial 39 finished with value: 0.0 and parameters: {'n_estimators': 7, 'reg_alpha': 0.6772189286604215, 'reg_lambda': 0.05686693888830778, 'colsample_bytree': 0.6, 'subsample': 0.8, 'learning_rate': 0.04804330048514166, 'max_depth': 4, 'num_leaves': 10, 'min_child_samples': 13}. Best is trial 0 with value: 0.0.\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\lightgbm\\engine.py:685: UserWarning: Found 'n_estimators' in params. Will use it instead of 'num_boost_round' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'num_boost_round' argument\")\n",
      "[I 2023-12-08 21:23:20,708] Trial 40 finished with value: 0.0 and parameters: {'n_estimators': 10, 'reg_alpha': 0.02233698325777631, 'reg_lambda': 0.3137428093302052, 'colsample_bytree': 0.3, 'subsample': 0.5, 'learning_rate': 0.002228710733941602, 'max_depth': 4, 'num_leaves': 7, 'min_child_samples': 19}. Best is trial 0 with value: 0.0.\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\lightgbm\\engine.py:685: UserWarning: Found 'n_estimators' in params. Will use it instead of 'num_boost_round' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'num_boost_round' argument\")\n",
      "[I 2023-12-08 21:23:38,632] Trial 41 finished with value: 0.0 and parameters: {'n_estimators': 29, 'reg_alpha': 0.05102297546954605, 'reg_lambda': 0.007665028302205793, 'colsample_bytree': 0.6, 'subsample': 0.9, 'learning_rate': 0.015707102014617523, 'max_depth': 5, 'num_leaves': 6, 'min_child_samples': 3}. Best is trial 0 with value: 0.0.\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\lightgbm\\engine.py:685: UserWarning: Found 'n_estimators' in params. Will use it instead of 'num_boost_round' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'num_boost_round' argument\")\n",
      "[I 2023-12-08 21:23:55,477] Trial 42 finished with value: 0.0 and parameters: {'n_estimators': 26, 'reg_alpha': 0.1316050661935878, 'reg_lambda': 0.0036182216581683285, 'colsample_bytree': 1.0, 'subsample': 0.9, 'learning_rate': 0.007389915342083874, 'max_depth': 6, 'num_leaves': 12, 'min_child_samples': 4}. Best is trial 0 with value: 0.0.\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\lightgbm\\engine.py:685: UserWarning: Found 'n_estimators' in params. Will use it instead of 'num_boost_round' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'num_boost_round' argument\")\n",
      "[I 2023-12-08 21:24:13,788] Trial 43 finished with value: 0.0 and parameters: {'n_estimators': 29, 'reg_alpha': 0.008416666908992444, 'reg_lambda': 0.002629027577913839, 'colsample_bytree': 0.6, 'subsample': 1.0, 'learning_rate': 0.06090252531199337, 'max_depth': 6, 'num_leaves': 11, 'min_child_samples': 4}. Best is trial 0 with value: 0.0.\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\lightgbm\\engine.py:685: UserWarning: Found 'n_estimators' in params. Will use it instead of 'num_boost_round' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'num_boost_round' argument\")\n",
      "[I 2023-12-08 21:24:30,787] Trial 44 finished with value: 0.0 and parameters: {'n_estimators': 27, 'reg_alpha': 0.23549855635928835, 'reg_lambda': 0.0014981845027855847, 'colsample_bytree': 0.6, 'subsample': 0.9, 'learning_rate': 0.014938958030873969, 'max_depth': 6, 'num_leaves': 11, 'min_child_samples': 6}. Best is trial 0 with value: 0.0.\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\lightgbm\\engine.py:685: UserWarning: Found 'n_estimators' in params. Will use it instead of 'num_boost_round' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'num_boost_round' argument\")\n",
      "[I 2023-12-08 21:24:38,382] Trial 45 finished with value: 0.0 and parameters: {'n_estimators': 11, 'reg_alpha': 0.15874006685501804, 'reg_lambda': 0.015801201790004993, 'colsample_bytree': 0.6, 'subsample': 0.4, 'learning_rate': 0.4384995090171532, 'max_depth': 4, 'num_leaves': 7, 'min_child_samples': 16}. Best is trial 0 with value: 0.0.\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\lightgbm\\engine.py:685: UserWarning: Found 'n_estimators' in params. Will use it instead of 'num_boost_round' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'num_boost_round' argument\")\n",
      "[I 2023-12-08 21:24:48,904] Trial 46 finished with value: 0.0 and parameters: {'n_estimators': 16, 'reg_alpha': 5.186819344986373, 'reg_lambda': 0.003378373986700506, 'colsample_bytree': 0.3, 'subsample': 0.7, 'learning_rate': 0.0037804500594461566, 'max_depth': 4, 'num_leaves': 18, 'min_child_samples': 12}. Best is trial 0 with value: 0.0.\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\lightgbm\\engine.py:685: UserWarning: Found 'n_estimators' in params. Will use it instead of 'num_boost_round' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'num_boost_round' argument\")\n",
      "[I 2023-12-08 21:24:53,539] Trial 47 finished with value: 0.0 and parameters: {'n_estimators': 6, 'reg_alpha': 0.030889763164527777, 'reg_lambda': 0.01998823249502994, 'colsample_bytree': 0.4, 'subsample': 0.5, 'learning_rate': 0.001987566556326019, 'max_depth': 4, 'num_leaves': 6, 'min_child_samples': 17}. Best is trial 0 with value: 0.0.\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\lightgbm\\engine.py:685: UserWarning: Found 'n_estimators' in params. Will use it instead of 'num_boost_round' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'num_boost_round' argument\")\n",
      "[I 2023-12-08 21:24:58,390] Trial 48 finished with value: 0.0 and parameters: {'n_estimators': 6, 'reg_alpha': 1.2584036138349355, 'reg_lambda': 0.0017910743879852792, 'colsample_bytree': 0.3, 'subsample': 0.7, 'learning_rate': 0.011863535773195148, 'max_depth': 4, 'num_leaves': 12, 'min_child_samples': 10}. Best is trial 0 with value: 0.0.\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\lightgbm\\engine.py:685: UserWarning: Found 'n_estimators' in params. Will use it instead of 'num_boost_round' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'num_boost_round' argument\")\n",
      "[I 2023-12-08 21:25:04,369] Trial 49 finished with value: 0.0 and parameters: {'n_estimators': 8, 'reg_alpha': 0.2017485771981724, 'reg_lambda': 0.0028628313004930763, 'colsample_bytree': 0.3, 'subsample': 0.7, 'learning_rate': 0.01967982278484429, 'max_depth': 5, 'num_leaves': 19, 'min_child_samples': 12}. Best is trial 0 with value: 0.0.\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\lightgbm\\engine.py:685: UserWarning: Found 'n_estimators' in params. Will use it instead of 'num_boost_round' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'num_boost_round' argument\")\n",
      "[I 2023-12-08 21:25:22,381] Trial 50 finished with value: 0.0 and parameters: {'n_estimators': 29, 'reg_alpha': 0.293415543509025, 'reg_lambda': 0.003856655730012103, 'colsample_bytree': 0.5, 'subsample': 0.9, 'learning_rate': 0.013690620511015135, 'max_depth': 6, 'num_leaves': 8, 'min_child_samples': 7}. Best is trial 0 with value: 0.0.\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\lightgbm\\engine.py:685: UserWarning: Found 'n_estimators' in params. Will use it instead of 'num_boost_round' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'num_boost_round' argument\")\n",
      "[I 2023-12-08 21:25:29,406] Trial 51 finished with value: 0.0 and parameters: {'n_estimators': 10, 'reg_alpha': 0.011001349457016384, 'reg_lambda': 7.6501259610282295, 'colsample_bytree': 0.3, 'subsample': 0.4, 'learning_rate': 0.0012674782125360084, 'max_depth': 4, 'num_leaves': 12, 'min_child_samples': 8}. Best is trial 0 with value: 0.0.\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\lightgbm\\engine.py:685: UserWarning: Found 'n_estimators' in params. Will use it instead of 'num_boost_round' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'num_boost_round' argument\")\n",
      "[I 2023-12-08 21:25:33,094] Trial 52 finished with value: 0.0 and parameters: {'n_estimators': 4, 'reg_alpha': 0.008595715726622873, 'reg_lambda': 1.9471496810925186, 'colsample_bytree': 0.8, 'subsample': 0.4, 'learning_rate': 0.0011820495826459648, 'max_depth': 5, 'num_leaves': 15, 'min_child_samples': 8}. Best is trial 0 with value: 0.0.\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\lightgbm\\engine.py:685: UserWarning: Found 'n_estimators' in params. Will use it instead of 'num_boost_round' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'num_boost_round' argument\")\n",
      "[I 2023-12-08 21:25:40,801] Trial 53 finished with value: 0.0 and parameters: {'n_estimators': 11, 'reg_alpha': 0.0034507238890194255, 'reg_lambda': 0.353926833185657, 'colsample_bytree': 0.8, 'subsample': 0.4, 'learning_rate': 0.005114566871321936, 'max_depth': 5, 'num_leaves': 11, 'min_child_samples': 3}. Best is trial 0 with value: 0.0.\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\lightgbm\\engine.py:685: UserWarning: Found 'n_estimators' in params. Will use it instead of 'num_boost_round' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'num_boost_round' argument\")\n",
      "[I 2023-12-08 21:25:48,688] Trial 54 finished with value: 0.0 and parameters: {'n_estimators': 11, 'reg_alpha': 0.004024310233159965, 'reg_lambda': 1.5376310521150194, 'colsample_bytree': 0.8, 'subsample': 0.9, 'learning_rate': 0.001759534055817027, 'max_depth': 5, 'num_leaves': 12, 'min_child_samples': 10}. Best is trial 0 with value: 0.0.\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\lightgbm\\engine.py:685: UserWarning: Found 'n_estimators' in params. Will use it instead of 'num_boost_round' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'num_boost_round' argument\")\n",
      "[I 2023-12-08 21:25:57,105] Trial 55 finished with value: 0.0 and parameters: {'n_estimators': 12, 'reg_alpha': 0.0022524722328129965, 'reg_lambda': 2.419855665863034, 'colsample_bytree': 0.8, 'subsample': 1.0, 'learning_rate': 0.0010780105491252922, 'max_depth': 5, 'num_leaves': 13, 'min_child_samples': 5}. Best is trial 0 with value: 0.0.\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\lightgbm\\engine.py:685: UserWarning: Found 'n_estimators' in params. Will use it instead of 'num_boost_round' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'num_boost_round' argument\")\n",
      "[I 2023-12-08 21:26:05,356] Trial 56 finished with value: 0.0 and parameters: {'n_estimators': 12, 'reg_alpha': 0.009307025569673659, 'reg_lambda': 8.744689890954827, 'colsample_bytree': 0.8, 'subsample': 0.7, 'learning_rate': 0.0013439912711653039, 'max_depth': 4, 'num_leaves': 10, 'min_child_samples': 8}. Best is trial 0 with value: 0.0.\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\lightgbm\\engine.py:685: UserWarning: Found 'n_estimators' in params. Will use it instead of 'num_boost_round' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'num_boost_round' argument\")\n",
      "[I 2023-12-08 21:26:13,765] Trial 57 finished with value: 0.0 and parameters: {'n_estimators': 12, 'reg_alpha': 3.946003087562164, 'reg_lambda': 0.40379094775967017, 'colsample_bytree': 0.7, 'subsample': 0.4, 'learning_rate': 0.010731536818151585, 'max_depth': 4, 'num_leaves': 18, 'min_child_samples': 3}. Best is trial 0 with value: 0.0.\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\lightgbm\\engine.py:685: UserWarning: Found 'n_estimators' in params. Will use it instead of 'num_boost_round' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'num_boost_round' argument\")\n",
      "[I 2023-12-08 21:26:25,816] Trial 58 finished with value: 0.0 and parameters: {'n_estimators': 18, 'reg_alpha': 4.220790843442801, 'reg_lambda': 0.5265589058747167, 'colsample_bytree': 0.7, 'subsample': 0.6, 'learning_rate': 0.03099667407183435, 'max_depth': 4, 'num_leaves': 14, 'min_child_samples': 3}. Best is trial 0 with value: 0.0.\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\lightgbm\\engine.py:685: UserWarning: Found 'n_estimators' in params. Will use it instead of 'num_boost_round' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'num_boost_round' argument\")\n",
      "[I 2023-12-08 21:26:31,838] Trial 59 finished with value: 0.0 and parameters: {'n_estimators': 8, 'reg_alpha': 2.159122763950032, 'reg_lambda': 0.0029654018897360386, 'colsample_bytree': 0.6, 'subsample': 0.4, 'learning_rate': 0.2706236809689735, 'max_depth': 5, 'num_leaves': 13, 'min_child_samples': 11}. Best is trial 0 with value: 0.0.\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\lightgbm\\engine.py:685: UserWarning: Found 'n_estimators' in params. Will use it instead of 'num_boost_round' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'num_boost_round' argument\")\n",
      "[I 2023-12-08 21:26:45,170] Trial 60 finished with value: 0.0 and parameters: {'n_estimators': 21, 'reg_alpha': 0.03478204028543207, 'reg_lambda': 2.2071537083901664, 'colsample_bytree': 0.8, 'subsample': 0.4, 'learning_rate': 0.0016046410789822012, 'max_depth': 6, 'num_leaves': 7, 'min_child_samples': 8}. Best is trial 0 with value: 0.0.\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\lightgbm\\engine.py:685: UserWarning: Found 'n_estimators' in params. Will use it instead of 'num_boost_round' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'num_boost_round' argument\")\n",
      "[I 2023-12-08 21:26:53,950] Trial 61 finished with value: 0.0 and parameters: {'n_estimators': 13, 'reg_alpha': 0.004985514438801666, 'reg_lambda': 2.813784447923329, 'colsample_bytree': 0.7, 'subsample': 0.7, 'learning_rate': 0.13376098537043232, 'max_depth': 4, 'num_leaves': 9, 'min_child_samples': 12}. Best is trial 0 with value: 0.0.\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\lightgbm\\engine.py:685: UserWarning: Found 'n_estimators' in params. Will use it instead of 'num_boost_round' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'num_boost_round' argument\")\n",
      "[I 2023-12-08 21:26:58,111] Trial 62 finished with value: 0.0 and parameters: {'n_estimators': 5, 'reg_alpha': 0.027430314952445314, 'reg_lambda': 2.0315091380662658, 'colsample_bytree': 0.7, 'subsample': 0.6, 'learning_rate': 0.10590118915266583, 'max_depth': 3, 'num_leaves': 10, 'min_child_samples': 9}. Best is trial 0 with value: 0.0.\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\lightgbm\\engine.py:685: UserWarning: Found 'n_estimators' in params. Will use it instead of 'num_boost_round' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'num_boost_round' argument\")\n",
      "[I 2023-12-08 21:27:06,013] Trial 63 finished with value: 0.0 and parameters: {'n_estimators': 11, 'reg_alpha': 0.0020353951442552056, 'reg_lambda': 1.842547174301404, 'colsample_bytree': 0.5, 'subsample': 0.4, 'learning_rate': 0.0012820031493397338, 'max_depth': 6, 'num_leaves': 13, 'min_child_samples': 4}. Best is trial 0 with value: 0.0.\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\lightgbm\\engine.py:685: UserWarning: Found 'n_estimators' in params. Will use it instead of 'num_boost_round' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'num_boost_round' argument\")\n",
      "[I 2023-12-08 21:27:11,464] Trial 64 finished with value: 0.0 and parameters: {'n_estimators': 7, 'reg_alpha': 0.04633158545291316, 'reg_lambda': 1.1585613678705171, 'colsample_bytree': 1.0, 'subsample': 0.7, 'learning_rate': 0.21411318576483282, 'max_depth': 3, 'num_leaves': 8, 'min_child_samples': 14}. Best is trial 0 with value: 0.0.\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\lightgbm\\engine.py:685: UserWarning: Found 'n_estimators' in params. Will use it instead of 'num_boost_round' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'num_boost_round' argument\")\n",
      "[I 2023-12-08 21:27:16,755] Trial 65 finished with value: 0.0 and parameters: {'n_estimators': 7, 'reg_alpha': 0.00831451824759876, 'reg_lambda': 0.8538323318219814, 'colsample_bytree': 0.7, 'subsample': 0.7, 'learning_rate': 0.16643142051641555, 'max_depth': 3, 'num_leaves': 7, 'min_child_samples': 7}. Best is trial 0 with value: 0.0.\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\lightgbm\\engine.py:685: UserWarning: Found 'n_estimators' in params. Will use it instead of 'num_boost_round' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'num_boost_round' argument\")\n",
      "[I 2023-12-08 21:27:24,028] Trial 66 finished with value: 0.0 and parameters: {'n_estimators': 10, 'reg_alpha': 0.004851681948267948, 'reg_lambda': 4.041315288522896, 'colsample_bytree': 0.8, 'subsample': 0.4, 'learning_rate': 0.0018862567815328077, 'max_depth': 5, 'num_leaves': 7, 'min_child_samples': 9}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning finished! \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/12/08 21:27:27 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\_distutils_hack\\__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "c:\\Users\\asus\\anaconda3\\envs\\ML\\Lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "mlflow_run_name =  \"LGBM\" \n",
    "model_name = \"lgbm\"\n",
    "direction = \"maximize\"\n",
    "task = \"classification\"\n",
    "# time allocated for each target in seconds\n",
    "timeout = 10*60 \n",
    "mlflow_model_name = \"lgbm\"\n",
    "n_splits = 3\n",
    "# to be specified for LGBM and XGBoost\n",
    "model_objective = \"binary\" \n",
    "# base score function in cross validation\n",
    "base_score_function = f1_score\n",
    "# LGBM and XGBoost cross validation score function\n",
    "def cv_score_function(preds, eval_data):\n",
    "    \"\"\"Returns score\n",
    "\n",
    "    Args:\n",
    "        eval_data (LGBM Dataset): ground truth data\n",
    "        preds (series): predictions\n",
    "    \n",
    "    \"\"\"\n",
    "    return(base_score_function.__name__, base_score_function(eval_data.get_label().astype(int), preds.astype(int)), True)\n",
    "cv_score_function.__name__ = base_score_function.__name__\n",
    "targets = cat_targets\n",
    "# calling main function\n",
    "score_dict, y_model_dict, opt_model_dict = tune_model(\n",
    "            run_name=mlflow_run_name, \n",
    "            model_name=model_name,\n",
    "            direction=direction,\n",
    "            task=task,\n",
    "            timeout=timeout,\n",
    "            targets=targets,\n",
    "            X_train=X_train, \n",
    "            y_train=y_train, \n",
    "            X_test=X_test, \n",
    "            y_test=y_test,\n",
    "            n_splits=n_splits,\n",
    "            random_state=random_state, \n",
    "            model_objective=model_objective,\n",
    "            base_score_function=base_score_function,\n",
    "            cv_score_function=cv_score_function,\n",
    "            mlflow_model_name=mlflow_model_name,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stator_yoke': {'lgbm': {'Accuracy': {'train': 0.8832098962773515,\n",
       "    'test': 0.8839929135641309},\n",
       "   'F1-micro': {'train': 0.8832098962773515, 'test': 0.8839929135641309},\n",
       "   'F1-macro': {'train': 0.8759150300177194, 'test': 0.8768837083633496},\n",
       "   'F1-weighted': {'train': 0.880229225528052, 'test': 0.8810881490666276}}}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
